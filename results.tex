\section{Experimentation}


% \textbf{Results:} The anchor-free YOLOv8-X model was the most accurate, achieving a mAP_{50} of 99.5\% and a mAP_{50:95} of 85.6\% on the test set with an inference time of 21.7ms, approximately 46 frames per second (FPS), demonstrating its effectiveness in real-time surgical tool detection. The most efficient model was YOLOv8-N, achieving a mAP_{50} of 99.5\% and a mAP_{50:95} of 83.3\% with 1.8ms inference time, approximately 555FPS (with only 3 million parameters - less than 20x the number in YOLOv8-X). The proposed tracking algorithm has an accuracy of 100\% over the detected tools and tooltips.

\subsection{In-House Dataset}

% describe the flow of participants throughout the study

% number of participants

outcome overall: Clear differences in some images (see differences in Test1 and Test2 scenes, and cut off images in Test2. Overheating in camera causing some image defects. edges more distinct, exact SOTA for opposite (YOLO in IRL, SIMO for ART)	 

% summary of follow-up

% overall characteristics of data source and setting

% we further used the best performin model to generate annotations for the rest of the dataset

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{schematic_diagram.png}
    \caption{Schematic diagram of the dataset files showing (a) folder structure for the dataset and (b) Data structure for the JSON label files.}
    \label{fig:schematic_diagram}
\end{figure}

Inspired by the CholecTrack20 schematic diagram \cite{nwoye_cholectrack20_2023}, we present an intuitive way to understand the dataset.

The average number of procedures performed was 120, with a range of 0 to over a 1000, with 22 estimated to be performed on average in the last 12 months, ranging from 0 to 100. All 24 participants agreed to share their results entirely. If we categorise the surgeons skill based on number of procedures, were low is <20, medium is between 20 and 100 and high is >100, then we have 9 low, 9 medium and 6 high skilled surgeons. Results from the Edinburgh Handedness survey \cite{oldfield_assessment_1971} showed almost all surgeons were right-handed, with one left (participant 12) and one who alternated hands (participant 3).

\subsubsection{Data Annotations}

% overall data annotated

% issues faced

In future, further annotations could be developed based on existing metrics of technical surgeons, e.g. Objective Structured Assessment of Technical Skills (OSATS) 8, Prostatectomy Assessment and Competency Evaluation (PACE) 9.
% 8 JA Martin, 10.1046/j.1365-2168.1997.02502.x 9 A Hussein, 10.1016/j.juro.2016.11.100
Though they are subjective, being based on questionnaires, they could be useful in settings a baseline for categorising surgical skill level 
% Hilal, Z.; Kumpernatz, A.K.; Rezniczek, G.A.; Cetin, C.; Tempfer-Bentz, E.-K.; Tempfer, C.B. A randomized comparison of video demonstration versus hands-on training of medical students for vacuum delivery using Objective Structured Assessment of Technical Skills (OSATS). Medicine 2017, 96, 11. [Google Scholar] [CrossRef]
we will identify key movement metrics which define the skill level for typical tasks in a laparoscopic procedure17. These metrics will be quantified from trainee videos to create task-specific learning curves.
% 17 M Pears, 10.1177/00369330211008594


\subsection{Model Configurations}

We have specifically designed the models with helper functions throughout the repository, freely available for assistance in future development.

\subsubsection{Anchor-based Model}

\subsubsection{Anchor-free Model}

\subsection{Training Setup}

\subsection{Model Evaluation}

% IoU, mAP50, mAP50-95, MSE

\subsection{Results}

% discuss differences between SIMO and YOLO models

\subsubsection{Surgical Tool Detection}

% IMAGE: bounding box tool and tooltips

% DIAGRAMS
\begin{table*}[h]
    \centering
    \caption{Object Detection Results on the AI-ELT Dataset.}
    \label{fig:modelresults}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \multicolumn{4}{|c|}{\textbf{Model}} & \multicolumn{2}{c|}{\textbf{mAP}} & \multicolumn{2}{c|}{\textbf{Inference}} & \multicolumn{3}{c|}{\textbf{Training}} \\
    \hline
    \textbf{Model Name} & \textbf{Anchor} $^a$ & \textbf{Params} $^b$ & \textbf{Layers} & \textbf{mAP$_{50}$} & \textbf{mAP$_{50:95}$} & \textbf{Time (ms)} & \textbf{FPS} & \textbf{Epochs} & \textbf{TT} $^c$ & \textbf{T/E} $^d$ \\ 
    \hline
    YOLOv10-X & AB & 31.7 & \textbf{688} & 99.0 & 81.0 & 19.3 & 52 & 19 & 7.8 & 0.41 \\ 
    YOLOv10-L & AB & 25.8 & 628 & 98.6 & 78.2 & 13.0 & 77 & 15 & 3.5 & 0.23 \\ 
    YOLOv10-B & AB & 20.5 & 518 & 98.85 & 80.8 & 10.3 & 97 & 14 & 2.2 & 0.16 \\ 
    YOLOv10-M & AB & 16.5 & 498 & 98.45 & 79.8 & 8.0 & 125 & 14 & 1.6 & 0.11 \\ 
    YOLOv10-S & AB & 8.1 & 402 & 99.25 & 80.9 & 3.9 & 256 & 16 & 1.8 & 0.11 \\ 
    YOLOv10-N & AB & 2.7 & 385 & 99.1 & 81.7 & 2.2 & 465 & 19 & 1.9 & 0.10 \\ 
    YOLOv8-X & AF & \textbf{68.2} & 385 & \textbf{99.5} & \textbf{85.6} & 21.7 & 46 & 84 & 18.6 & 0.22 \\ 
    YOLOv8-L & AF & 43.6 & 365 & \textbf{99.5} & 84.1 & 12.8 & 78 & 36 & 1.6 & 0.05 \\ 
    YOLOv8-M & AF & 25.9 & 295 & \textbf{99.5} & 84.1 & 8.0 & 125 & 29 & 1.7 & 0.06 \\ 
    YOLOv8-S & AF & 11.1 & 225 & \textbf{99.5} & 85.0 & 3.4 & 294 & 66 & 1.0 & \textbf{0.02} \\ 
    YOLOv8-N & AF & 3.0 & 225 & \textbf{99.5} & 83.3 & \textbf{1.8} & \textbf{556} & 41 & \textbf{0.8} & \textbf{0.02} \\ 
    RetinaDet & AB & 36.4 & 195 & \textbf{99.9} & 77.2 & 5.1 & 196 & 99 & 25.7 & 0.26 \\ 
    RetinaDet-Opt & ABO & 36.4 & 195 & \textbf{99.9} & 77.5 & 5.2 & 192 & 25 & 2.2 & 0.09 \\ 
    EfficientDet & AB & 6.6 & 552 & 48.5 & 33.7 & 4.0 & 250 & 162 & 4.8 & 0.03 \\ 
    SIMO-Resnet & AF & 23.5 & 231 & 14.05 & 12.15 & 350.0 & 3 & 16 & 1.3 & 0.08 \\ 
    SIMO-VGG & AF & 14.7 & 98 & 17.75 & 14.4 & 1280.0 & 1 & 26 & 13.0 & 0.50 \\ 
    DETR & AF & 41.5 & 318 & 49.3 & 39.3 & 35.9 & 28 & \textbf{294} & 16.2 & 0.06 \\
    \hline
\end{tabular}
% go to next line
\newline
\footnotesize{$^a$ Anchor: AB = Anchor-based, AF = Anchor-free, ABO = Anchor-based with anchor box optimisation. $^b$ Trainable parameters in millions. $^c$ Training Time (in hours). $^d$ Time per epoch (in hours).}
\end{table*}

\subsubsection{Surgical Tool Tracking}

% IMAGE: show tracking of tool (bounding boxes over time)

% DIAGRAMS

\subsubsection{Performance of the Model}

YOLO10 architectures
tested with all data - overfit too easily

SIMO - initial issues, like segmentation, changed to detection with confidence values, added FCN

main discoveries

\subsubsection{Comparison to Previous Work}

\subsubsection{Qualitative Results}

Summary of what we saw in the results. main discoveries