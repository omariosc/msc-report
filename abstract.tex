\begin{abstract}
    \textbf{Background:} In low-middle-income countries (LMICs), there is a critical need for skilled surgeons. Alternative training processes that include computer-assisted surgical skill evaluation are essential to address this gap. Leveraging surgical videos to derive insights into tool detection and tracking is a highly researched area. This study focuses primarily on the detection of surgical tools in laparoscopic datasets with a secondary focus on tool tracking, aiming to develop a generalizable model applicable to non-in vivo contexts.

    \textbf{Objectives:} The primary objective is to build accurate models for tool detection in laparoscopic videos. These models are evaluated against comparable datasets, such as the ART-Net and MICCAI 2015 Endoscopic Vision Challenge (EndoVis 2015) datasets, to benchmark performance and visualise results.
 
    \textbf{Methods:} We collected an in-house dataset comprising 24 laparoscopic surgery videos, each approximately 3 minutes long in 1920x1080 resolution, totalling 1.32GB. These videos feature procedures performed by either a trainee or an expert trainer. The dataset includes 103,629 frames annotated with tooltips and ground truth data for tool position in three dimensions, including quaternions for spatial orientation and rotation values (6 degrees of freedom), and complete bounding box annotations for tools and tooltips. Multiple anchor-based and anchor-free deep learning models, including various state-of-the-art (SOTA) models, are implemented and tested. These models are evaluated using the standard Common Objects in Context (COCO) metric, yielding mean Average Precision (mAP) scores for detected bounding boxes around the tools and tooltips at a 0.5 intersection over union (IoU) threshold. The models are trained on a single NVIDIA GeForce RTX 3060 GPU with 12GB of GPU memory.
  
    \textbf{Results:} The best-produced model, the anchor-free YOLOv8-X model, achieved a mAP50 of 99.5\% and a mAP50-95 of 85.6\% on the test set with an inference time of 21.7ms, approximately 46 frames per second (FPS), demonstrating its effectiveness in real-time surgical tool detection. The proposed tracking algorithm has an accuracy of 100\% over the detected tools and tooltips. These results highlight the potential of the models for accurate tool localization, even in resource-constrained environments.
  
    \textbf{Discussion:} The proposed models and dataset are particularly useful for applications in LMICs where access to advanced training facilities is limited, considering our data is comparable to what would be obtained in a real-world training setting. The spatial data combined with our results from tool detection and tracking can be developed upon in further research to build models for more precise surgical tool pose estimation, workflow recognition and skill assessment.
  
    \textbf{Repository:} The code, models and checkpoints are available at \url{https://github.com/omariosc/msc-surgical-tool-tracking/}.
  \end{abstract}