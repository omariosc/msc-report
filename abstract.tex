\begin{abstract}
    \textbf{Background:} In low-middle-income countries (LMICs), there is a critical need for skilled surgeons. Alternative training processes that include computer-assisted surgical skill evaluation are essential to address this gap. Leveraging surgical videos to derive insights into tool detection and tracking is a highly researched area. This study focuses on detecting and tracking surgical tools in a new proposed dataset, AI-ELT (Artificial Intelligence Enhanced Laparoscopic Training), aiming to develop a generalisable model applicable to non-in vivo contexts.

    \textbf{Objectives:} The primary objective is to compare state-of-the-art (SOTA) tool detection models for laparoscopic videos on the AI-ELT dataset. This will help inform us which models are best suited for future research in non-in-vivo laparoscopic datasets. The areas of interest are comparing anchor-based and anchor-free methods, varying model sizes and number of layers and parameters, tool vsitnootlp detiction, efe rence times, and training times. The secondary objective is to produce an accurate tracking algorithm for the AI-ELT dataset to help the annotation process.
 
    \textbf{Methods:} We collected an in-house dataset comprising 24 laparoscopic surgery videos, each approximately 3 minutes long in 1920x1080 resolution, totalling 1.32GB. These videos feature procedures performed by either a trainee or an expert trainer. The dataset includes 103,629 frames annotated with tooltips and six degrees of freedom ground truth data for three-dimensional tool positions and quaternions for spatial rotation, with complete bounding box annotations for tools and tooltips. Multiple anchor-based and anchor-free deep learning models are implemented and tested, including various SOTA models, namely YOLOv10, YOLOv8, RetinaNet, EfficientDet, DETR and ART-Net. These models are evaluated using the standard Common Objects in Context (COCO) metric, yielding mean Average Precision (mAP) scores for detected bounding boxes around the tools and tooltips. The models are trained on a single NVIDIA GeForce RTX 3060 GPU with 12GB of GPU memory.
  
    \textbf{Results:} Overall, the anchor-free YOLOv8-N model was the most accurate and efficient model, achieving mAP$_{50}$ of 99.5\% and mAP$_{50:95}$ of 83.3\% on the test set with an inference time of 1.8ms, approximately 556 frames per second (FPS), using ~3 million parameters. The proposed tracking algorithm has an accuracy of 100\% over the detected tools and tooltips.
  
    \textbf{Discussion:} These results highlight the model's potential for effective real-time surgical tool detection, even in resource-constrained environments. The proposed models and dataset are particularly useful for applications in LMICs where access to advanced training facilities is limited, considering our data is comparable to what would be obtained in a real-world training setting. The spatial and skill data unused in this study, combined with our tool detection and tracking results, can be developed in further research to build models for more precise surgical tool pose estimation, 3D reconstruction, workflow recognition and skill assessment.
  
    \textbf{Repository:} The code, models and checkpoints are available at \url{https://github.com/omariosc/msc-surgical-tool-tracking/}.
  \end{abstract}