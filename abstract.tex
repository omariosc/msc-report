\begin{abstract}
    \textbf{Background:} In low-middle-income countries (LMICs), there is a critical need for skilled surgeons. Alternative training processes that include computer-assisted surgical skill evaluation are essential to address this gap. Surgical videos can be leveraged to derive insights into surgical skill assessment using tool detection and tracking, both highly researched areas.
 
    \textbf{Methods:} Multiple anchor-based and anchor-free deep learning SOTA models are implemented and tested, yielding mean Average Precision (mAP) scores for detected tool and tooltip bounding boxes across the newly proposed in-house AI-ELT dataset.
    
    \textbf{Results:} Overall, the anchor-free YOLOv8-X model was the most accurate and efficient, achieving mAP$_{50}$ of 99.5\% and mAP$_{50:95}$ of 85.6\% on the test set with an inference time of 21.7ms, approximately 46 FPS. The proposed tracking algorithm has an accuracy of 100\% over given detections.
  
    \textbf{Discussion:} The introduced dataset is particularly useful for applications in LMICs where access to advanced training facilities is limited. These results highlight the models' potential for effective real-time surgical tool detection, even in resource-constrained environments. 
\end{abstract}

% \begin{abstract}
%     \textbf{Background:} In low-middle-income countries (LMICs), there is a critical need for skilled surgeons. Alternative training processes that include computer-assisted surgical skill evaluation are essential to address this gap. Surgical videos can be leveraged to derive insights into surgical skill assessment using tool detection and tracking, both highly researched areas. The primary objective of this study is to detect and track surgical tools using various state-of-the-art (SOTA) models on the in-house Artificial Intelligence Enhanced Laparoscopic Training (AI-ELT) dataset to aid future research in non-in-vivo laparoscopic datasets in resource-constrained environments.
 
%     \textbf{Methods:} We collected an in-house dataset comprising 24 laparoscopic surgery videos of a peg transfer task, each approximately 3 minutes long in 1920x1080 resolution, totalling 1.32GB. These videos feature procedures performed by either a trainee or an expert trainer. The dataset includes 103,629 frames annotated with six degrees of freedom - ground truth data for three-dimensional tool positions and quaternions for spatial rotation, with partially labelled bounding box annotations. Multiple anchor-based and anchor-free deep learning SOTA models are implemented and tested, namely YOLOv10, YOLOv8, RetinaNet, EfficientDet, ART-Net and DETR, evaluated using the standard Common Objects in Context (COCO) metric, yielding mean Average Precision (mAP) scores for detected tool and tooltip bounding boxes. The models are trained on a single NVIDIA GeForce RTX 3060 GPU with 12GB of GPU memory.
  
%     \textbf{Results:} Overall, the anchor-free YOLOv8-X model was the most accurate and efficient, achieving mAP$_{50}$ of 99.5\% and mAP$_{50:95}$ of 85.6\% on the test set with an inference time of 21.7ms, approximately 46 frames per second (FPS), using ~68 million parameters. The proposed tracking algorithm has an accuracy of 100\% over given detections.
  
%     \textbf{Discussion:} These results highlight the models' potential for effective real-time surgical tool detection, even in resource-constrained environments. The proposed models and dataset are particularly useful for applications in LMICs where access to advanced training facilities is limited, considering our data is comparable to what would be obtained in a real-world training setting. The spatial and skill data unused in this study, combined with our tool detection and tracking results, can be developed in further research to build models for more precise surgical tool pose estimation, 3D reconstruction and tracking, workflow recognition and skill assessment.
  
%     \textbf{Repository:} The code and models are available at \url{https://github.com/omariosc/msc-surgical-tool-tracking/}.
%   \end{abstract}