\section{Background Research}

\subsection{Identification and Evaluation of Relevant Datasets}

% https://link.springer.com/article/10.1007/s11263-022-01640-6

We identified 64 surgical datasets, of which we categorised 5 for tracking (where there is video access or subsequent frames available with appropriate annotations), 4 for pose estimation (with ground truth sensor data), 6 for detection (only bounding box annotations available), 10 for skill (classification of the operators' surgical skills), 11 for workflow analysis (the current sub-procedure or workflow carried out by the surgeon), 20 for segmentation (with full tool masks) and 7 for other use. Since some datasets have multiple use cases, we organised them so that if we could use them for tracking then we considered it to a dataset for tracking. In total, 37 datasets were identified as being useful with 9 excluded as they were based on robotic surgery (not relevant to us), with the remaning 18 datasets being excluded as they were not relevant. Only 7 datasets were ready for immediate use, with a further 5 available after requesting access.

Some datasets had more value as the quality of images and annotations were greater and very few had code available for direct use. We list these datasets as follows: MICCAI 2015 Endoscopic Vision Challenge (EndoVis 2015), MICCAI 2016 Endoscopic Vision Challenge (m2cai16), MICCAI 2024 Endoscopic Vision Challenge (SurgVU) \footnote{Based on the previous 2022 and 2023 challenge versions.}, PEg TRAnsfer Workflow Recognition by different modalities (PETRAW) \footnote{Sub-challenge as a part of MICCAI 2021} and the Augmented Reality Tool Network (ART-Net) dataset. Of these, the most relevant datasets were PETRAW as it is the ideal quality of the dataset that we would expect ours to be and ART-Net as it contained segmentation masks for tooltips which is something most datasets lacked.

Tools go in and out of frame, and the appearance of the tool changes as it moves around and is rotated. A dataset should include these different cases.

% ground truth for 3-D position and orientation is not available for in vivo data https://ieeexplore.ieee.org/abstract/document/6359786

% http://dx.doi.org/http://dx.doi.org/10.1016/j.media.2017.01.007 MIS operations are highly complex and the surgeon must deal with a difficult hand-eye coordination, a restricted mobility and a narrow field of view. This will inevitably result in a lower degree of data.

% The authors acknowledge several limitations in the current state of research. One major issue is the lack of publicly available high-quality datasets, which hampers the development and benchmarking of new methods. The robustness and reproducibility of many approaches are often challenged by varying lighting conditions, occlusions, and the complex dynamics of surgical environments. Furthermore, many methods still rely on empirical thresholds and criteria, which can limit their generalizability and adaptability to different surgical scenarios. The review also points out the need for more research into real-time implementation and the integration of these technologies into clinical workflows. https://arxiv.org/abs/2209.01435

% avoiding segmentation

\subsubsection{Augmented Reality Tool Network (ART-Net) Dataset}

% ART-NET: If there is enough time, we could try the method on another dataset. For ART-Net, we could convert the segmentation map into a bounding box, for the entire tool and just for the tool tip. I could consider other public datasets and what has been benchmarked already and how, so that I could employ the same method for comparison.
% This dataset consists non-robotic tools with annotated tool presence, tool segmentation, and instrumnt geometric primitives (mid-line, edge-line, tooltip). The images come from laparoscopic hysterectomy videos. This dataset also contains tool presence annotated for another set of 3000 images, namely 1500 positive and 1500 negative images, respectively, for which some positive images contain multiple tools. 4270 images are labelled for tool detection. If the tool shaft is not visible at all, the image is marked as negative. When a small part of the tool shaft is visible, the image is marked as positive. For segmentation and geometric primitive extraction, 635 images are annotated.
% The ART-Net dataset (Hasan et al., 2021) is tailored for nonrobotic laparoscopic hysterectomy, emphasizing 3D graphics applications. Annotations cover tool presence detection, binary tool segmentation, and 2D pose estimation. Extracted from 29 procedures, the dataset provides frames with and without instruments, annotating these frames for tool presence. Keyframes are annotated for binary tool segmentation and 2D pose in the form of geometric primitives (tool-tip, midline and instrument shaft heatmaps). https://arxiv.org/abs/2401.08256
% This project proposed ART-Net to detect, segment, and extract three geometric primitives simultaneously from the laparoscopic images. These primitives are the tool edge-lines, mid-line, and tip. They allow the tool's 3D pose to be estimated by a fast algebraic procedure. The framework only proceeds if a tool is detected. The accuracy of segmentation and geometric primitive extraction is boosted by a new Full resolution feature map Generator (FrG). https://github.com/kamruleee51/ART-Net

% Further work has been done on this dataset in segmentation \cite{lou_min-max_2022} but in classical object detection.

% https://link.springer.com/article/10.1007/s11548-024-03125-y

\subsubsection{PEg TRAnsfer Workflow Recognition by different modalities (PETRAW) Dataset}

% PETRAW: The PETRAW dataset (Huaulme et al., 2023) is designed for workflow recognition in peg transfer training sessions. The dataset comprises 150 sequences of peg transfer sessions recorded on a virtual reality simulator, kinematic data, videos, and annotations for semantic segmentation (2 targets and 1 instrument), phase (2 phases), step (12 steps), and action annotations (6 actions). https://arxiv.org/abs/2401.08256
% https://www.synapse.org/Synapse:syn25147789/wiki/608848
% https://arxiv.org/abs/2202.05821

\subsubsection{MICCAI 2015 Endoscopic Vision Challenge (EndoVis 2015) Dataset}

% The EndoVis challenge is a high-profile international challenge for the comparative validation of endoscopic vision algorithms that focus on different problems each year. One of the most used for surgical instruments is the EndoVis 2015, which is a multi-instrument dataset, split into training and test data, with the training data including four 45-second intervention videos, and the test set being comprised of additional 15-seconds video sequences for each of the training sequences and two additional 1-minute recorded interventions. The resolution of all sequences is 720×576 pixels. https://ieeexplore.ieee.org/abstract/document/10253772

\subsubsection{Potential to Collect New Datasets}

% training datasets are fewer, easier to collect and to be used in LMIC

% generate yourself https://www.tandfonline.com/doi/full/10.1080/21681163.2020.1835546?src=recsys

% https://link.springer.com/article/10.1007/s11263-022-01640-6

\subsection{State-of-the-Art Object Detection Methods}

Much work has been done in the field of object detection.

We will not discuss the exact performance of these models nor compare them against each other directly here as there are various context and different metrics used. We will have further discussions on this later in the paper.

\subsubsection{You Only Look Once (YOLO)}

% frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. YOLO learns very general representations of objects. A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes. YOLO trains on full images and directly optimizes detection performance. This unified model has several benefits over traditional methods of object detection. Unlike classifier-based approaches, YOLO is trained on a loss function that directly corresponds to detection performance and the entire model is trained jointly. YOLO also generalizes well to new domains making it ideal for applications that rely on fast, robust object detection. https://arxiv.org/abs/1506.02640
% YOLO: Redmon et al.[19] proposed YOLO (You Only Look Once) algorithm in 2016. As the earliest one-stage detection algorithm, it treats the object detection task as a regression problem, and predicts the coordinates of the bounding box, the categories of objects contained in the bounding box and the confidence of classification by directly processing the whole image. https://www.sciencedirect.com/science/article/pii/S0921889021002232

% Anchor-based is interesting because videos often have a wide range of aspect ratios, etc. (insert examples). We decided that it would be best to continue with the current dataset, and to make it scientific and sufficient for the MSc, I would look at investigating anchor-based methods (e.g. YOLO) and anchor-less methods (fully connected one-stage CNN). A method for each would be sufficient. The idea is that the aspect ratio of the tool will change as it moves around and is rotated, with different orientations and scales in various ways, so anchors in anchor-based methods would need to be optimised. I would look at what methods could cope with those situations and what their limitations are.

% Non-maximum suppression (NMS) is a technique used in object detection to remove overlapping bounding boxes. It is a post-processing algorithm that filters out boxes with low confidence scores and high IoU (Intersection over Union) with other boxes. NMS is a crucial step in object detection pipelines to ensure that the model predicts only one bounding box for each object. The algorithm works by selecting the box with the highest confidence score and removing all other boxes that have a high IoU with the selected box. This process is repeated until all boxes are processed. NMS is an essential step in object detection pipelines to ensure that the model predicts accurate bounding boxes for objects in the image. https://www.pyimagesearch.com/2020/08/10/opencv-object-tracking/

% The execution time of NMS primarily depends on the number of boxes and two thresholds. As the confidence threshold increases, more prediction boxes are filtered out, and the number of remaining boxes that need to calculate IoU decreases, thus reducing the execution time of NMS. Another observation is that anchor-free detectors outperform anchor-based detectors with equivalent accuracy for YOLO detectors because the former require less NMS time than the latter.

% YOLOv10: YOLOs have emerged as the predominant paradigm in the field of real-time object detection owing to their effective balance between computational cost and detection performance. Researchers have explored the architectural designs, optimization objectives, data augmentation strategies, and others for YOLOs, achieving notable progress. consistent dual assignments for NMS-free training of YOLOs, which brings the competitive performance and low inference latency simultaneously. Moreover, we introduce the holistic efficiency-accuracy driven model design strategy for YOLOs. We comprehensively optimize various components of YOLOs from both the efficiency and accuracy perspectives, which greatly reduces the computational overhead and enhances the capability achieves the stateof-the-art performance and efficiency across various model scales.
% we target both the post-processing and model architecture throughout the detection pipeline of YOLOs. For the post-processing, we propose the consistent dual assignments for NMSfree training, achieving efficient end-to-end detection. For the model architecture, we introduce the holistic efficiency-accuracy driven model design strategy, improving the performance-efficiency tradeoffs. These bring our YOLOv10, a new real-time end-to-end object detector. Extensive experiments show that YOLOv10 achieves the state-of-the-art performance and latency compared with other advanced detectors, well demonstrating its superiority. https://arxiv.org/pdf/2405.14458
% https://arxiv.org/abs/2405.14458#

% A YOLO-based model was presented by Choi et al. [20]. His work reported the fastest inference time of 48 FPS in the m2cai16-tool-location dataset but low performance for localization over preselected videos for validation. Choi, B., Jo, K., Choi, S., Choi, J.: Surgical-tools detection based on convolutional neural network in laparoscopic robot-assisted surgery. In: Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS, pp.  1756–1759. IEEE, Piscataway, NJ (2017)

% "We trained two state-of-the-art detectors, RetinaNet and YOLOv2, with bounding boxes centered around the tip annotations with specific margin sizes to determine the optimal margin size for detecting the tip of the instrument and localizing the point. https://www.sciencedirect.com/science/article/pii/S0010482521001785

% https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/htl2.12072 looked at "This study focuses on enhancing the inference speed of laparoscopic tool detection on embedded devices. Laparoscopy, a minimally invasive surgery technique, markedly reduces patient recovery times and postoperative complications. Real-time laparoscopic tool detection helps assisting laparoscopy by providing information for surgical navigation, and its implementation on embedded devices is gaining interest due to the portability, network independence and scalability of the devices. However, embedded devices often face computation resource limitations, potentially hindering inference speed. To mitigate this concern, the work introduces a two-fold modification to the YOLOv7 model: the feature channels and integrate RepBlock is halved, yielding the YOLOv7-RepFPN model. This configuration leads to a significant reduction in computational complexity. Additionally, the focal EIoU (efficient intersection of union) loss function is employed for bounding box regression. Experimental results on an embedded device demonstrate that for frame-by-frame laparoscopic tool detection, the proposed YOLOv7-RepFPN achieved an mAP of 88.2% (with IoU set to 0.5) on a custom dataset based on EndoVis17, and an inference speed of 62.9 FPS. Contrasting with the original YOLOv7, which garnered an 89.3% mAP and 41.8 FPS under identical conditions, the methodology enhances the speed by 21.1 FPS while maintaining detection accuracy. This emphasizes the effectiveness of the work. A detection speed of 25–30 FPS is generally seen as real-time [10, 11]."

% https://www.mdpi.com/2076-3417/9/14/2865 Firstly, the proposed method can detect surgical tools in real time by using the object detection system YOLO9000. Unlike other methods, You Only Look Once (YOLO) does not allow for finding the region of interest (ROI). Conventional methods aim to identify the ROI from an input image and thereafter, to classify each ROI. However, applying YOLO allowed for the diminishing of the time required to calculate the ROI. YOLO divides an input image into a set of grid cells and then, performs classification of each grid cell. Owing to this key feature of YOLO, the proposed algorithm can detect surgical tools in real time (Table 3).

% https://d197for5662m48.cloudfront.net/documents/publicationstatus/182926/preprint_pdf/c01200a120dadb10c17594a11edcded8.pdf We found that the performance of all the NAS-based YOLO was inferior as compared to other State-of-the-Art (SoTA) YOLO models. We compare our results against the YOLOv7 model too.

% https://www.mdpi.com/1424-8220/24/13/4191 "The You Only Look Once (YOLO) algorithm [15] implements this unified approach by framing object detection as a regression problem. YOLO is recognized as one of the most efficient algorithms, suitable for real-time processing, due to its single convolutional network evaluation. The YOLO architecture was introduced by Choi et al. [16] for real-time surgical instrument tracking. They achieved 72.26% mean average precision on a dataset that included seven surgical tools. The convolutional layers were pretrained using the ImageNet 1000-class competition dataset, and then a gallbladder surgery image dataset was used for the learning process. Choi et al. finally concluded that the low precision in the detection of some specific instruments was due to the insufficient number of images to learn from. Although single-stage detectors such as YOLO show more efficiency than their twostage counterparts, both approaches rely on anchor boxes. These anchor boxes introduce numerous hyperparameters that require fine-tuning, hindering the network training process. Despite their high accuracy in surgical tool detection, methods utilizing anchor boxes often fall short in real-time applications. To address this limitation, Liu et al. [17] combined an anchor-box-free CNN with an Hourglass network [18], facilitating real-time surgical tool localization through heatmap estimation. Models based on U-Net architectures [19], widely popular in segmentation tasks, have also been used to determine the position of surgical instruments [20,21]. Kurmann et al. proposed a U-shaped network to simultaneously recognize multiple instruments and their parts [22]. Laina et al. [23] formulated the position estimation task as heatmap regression, estimated concurrently with tool segmentation."

% https://ieeexplore.ieee.org/abstract/document/10168997 Several approaches have been developed to achieve bounding box tool classification. One interesting implementation examined the performance of a You Only Look Once (YOLO) based network [5] across 7 different types of tools [6]. YOLO based networks boast very high performance speeds, making them an ideal choice for real time applications. However, with the lack of optimisation, they underperform in harsh environments such as the ones associated to laparoscopic or orthopaedic operations. To address this, a YOLO900 based network [7] was constructed to take into account motion prediction from previous frames in order to improve detection performance [8]. This use of temporal information, however, can lead to an exponentially increasing error, since the success of tool detection in the current frame depends on the success of previous detections. Nevertheless, this approach further underlines the high speed and tunability of YOLO based networks. These networks, however, are not the only ones that demonstrate high detection speeds. It has been shown that it is possible to achieve extremely fast box detection by constructing non-region-based networks for tool detection. Specifically, the Extremely Fast and Precise Network (EF- PNet) [9] was constructed with the aim of optimising detection speed, achieving 270 fps in detection. This research suggests that in a surgical context, YOLO based networks may not be optimal for box detection. A similar process was explored when developing a box detection network that did not require the formulation of anchors [10]. Even though the inference speed only ran at 37 fps, the accuracy was increased significantly. Another advantage of deep learning approaches was the option to integrate spatio-temporal data across the detection process to improve results. A Spatial Transformer Network (STN) has been combined with a CNN to detect tools moving at high speed, a condition which usually suffers from erroneous detections due to image blur [11]. However, occlusions significantly impact such methods.

\subsubsection{ART-Net Model}

% ART-Net: Problem is the model was built for segmentation. To convert this into a detection problem would significantly reduce the accuracy of the model. Though we can easily convert segmentation masks into bounding box annotations for the ART-Net dataset, annotating our dataset to be used for segmentation would be very time-consuming and not relevant as discussed in the introduction.
% ART-Net achieves in both average precision and accuracy. In segmentation, it achieves in mean Intersection over Union (mIoU) on the robotic EndoVis dataset (articulated tool), \cite{hasan_detection_2021}. The proposed framework outperforms existing ones in detection and segmentation. Compared to separate networks, integrating the tasks in a single network preserves accuracy in detection and segmentation but substantially improves accuracy in geometric primitive extraction. 
% ART-Net has a single encoder and five sub-network branches, namely one for tool detection, one for tool segmentation, and three for geometric primitive extraction.

% developed a CNN they called ART-Net, for Augmented Reality Tool Network, and combined it with an algebraic geometry approach for generic tool detection, segmentation, and 3D pose estimation. While the CNN ART-Net was used for surgical tool detection and segmentation, geometric primitives were also extracted to compute the 3D pose with algebraic geometry.

\subsubsection{RetinaNet}

% RetinaNet: Retina-net [21] is the latest one-stage detection framework, which puts forward focus loss for unbalanced categories. It inhibits the categories with more and easier classification in the loss function and greatly improves the proportion of losses with less and difficult classification. This network structure has significant advantages in accuracy, efficiency and complexity. https://www.sciencedirect.com/science/article/pii/S0921889021002232
% Based on ResNet He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016) https://arxiv.org/abs/1512.03385

We can use RetinaNet with anchor-box optimisation \cite{zlocha2019improving}.

% \subsubsection{EfficientDet}

\subsubsection{DEtection TRansformer (DETR)}

% DETR: views object detection as a direct set prediction problem. \cite{vedaldi_end--end_2020}. streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task.
% Many Deep Learning based methods such as Fast-RCNN have achieved SOTA performance in tool detection and localization (Du et al, 2018a) but are computationally expensive, introducing inference time penalties. https://arxiv.org/abs/2209.01435.
% The goal of object detection is to predict a set of bounding boxes and category labels for each object of interest. Modern detectors address this set prediction task in an indirect way, by defining surrogate regression and classification problems on a large set of proposals [37,5], anchors [23], or window centers [53,46].
% 23: Lin, T.Y., Goyal, P., Girshick, R.B., He, K., Doll´ar, P.: Focal loss for dense object detection. In: ICCV (2017) https://arxiv.org/abs/1708.02002v2
% 37: Ren, S., He, K., Girshick, R.B., Sun, J.: Faster R-CNN: Towards real-time object detection with region proposal networks. PAMI (2015)
% 5: Cai, Z., Vasconcelos, N.: Cascade R-CNN: High quality object detection and instance segmentation. PAMI (2019)
% 53. Zhou, X., Wang, D., Kr¨ahenb¨uhl, P.: Objects as points. arXiv:1904.07850 (2019)
% 46. Tian, Z., Shen, C., Chen, H., He, T.: FCOS: Fully convolutional one-stage object detection. In: ICCV (2019)
% DETR directly predicts (in parallel) the final set of detections by combining a common CNN with a transformer architecture [47]. During training, bipartite matching uniquely assigns predictions with ground truth boxes. 
% 47:. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017)
% DETR uses a conventional CNN backbone to learn a 2D representation of an input image. The model flattens it and supplements it with a positional encoding before passing it into a transformer encoder. A transformer decoder then takes as input a small fixed number of learned positional embeddings, which we call object queries, and additionally attends to the encoder output. We pass each output embedding of the decoder to a shared feed forward network (FFN) that predicts either a detection (class and bounding box) or a “no object” class.
% achieves competitive results compared to Faster R-CNN in quantitative evaluation on COCO.
% resnet backbone https://arxiv.org/abs/1512.03385

% Loza: we use advancements in \cite{Loza2023DTx}. The proposal is to utilize multi-scale features within thefeature extraction layer and at the transformer-based detection architecture through posi-tional encoding that can reﬁne and capture context-aware and structural information ofdifferent-sized tools. Furthermore, a supervised contrastive loss is introduced to optimizerepresentations of object embeddings, resulting in improved feed-forward network perfor-mances for classifying localized bounding boxes. The strategy demonstrates superiority tostate-of-the-art (SOTA) methods. Generation of richer features through incorporating aRes2Net [https://doi.org/10.1109/TPAMI.2019.2938758] as backbone, an architecture that makeslocal-scale consideration for the extraction of features.∙ Multi-scale position encoding of two projected features mapsextracted from the backbone to incorporate features at multi-ple scales in the self-attention mechanism of the transformer.We call this new architecture our proposed “’dense trans-former” (DTX) network and it is inspired by the DETRdetector \cite{vedaldi_end--end_2020}.∙ Contrastive learning over the object representation of the sur-gical tools to encourage consistency and separability in thefeature embeddings of the different classes


\subsection{Tracking Methods}

% Secondary focus is tracking, the main focus is on detection. Tracking can be done if IDs are tracked and we consider tools moving out of frame (their appearance, disappearance and reappearance). Segmentation is not needed as tracking is our aim, not pixel-wise classification.

% Issues such as blur and occlusion can affect tracking results https://link.springer.com/chapter/10.1007/978-3-319-46720-7_49 as well as when objects are small or move out of frame https://link.springer.com/article/10.1007/s11548-024-03246-4.

% more annotated data might be the key to improve results of the machine learning based tracking methods, but acquiring large quantities of training data is challenging. https://arxiv.org/pdf/1805.02475

% Many SOTA methods including ByteTrack https://arxiv.org/abs/2110.06864, BoT-SORT https://arxiv.org/abs/2206.14651, SMILEtrack https://arxiv.org/abs/2211.08824, DeepSort % https://d197for5662m48.cloudfront.net/documents/publicationstatus/182926/preprint_pdf/c01200a120dadb10c17594a11edcded8.pdf We found that the performance of all the NAS-based YOLO was inferior as compared to other State-of-the-Art (SoTA) YOLO models. We compare our results against the YOLOv7 model too.
% since our data is well known, we can use a simple tracking algorithm appropariate for our data. With perfect accuracy we can develop annotations to use more standard methods in future work.

% many different algorithms, and list some (DeepSORT, ByteTrack, BotSORT) however we will create our own algorithm for us on datasets similar to PETRAW and our dataset.

Currently, tool detection methods primarily rely on tracking-by-detection, with a reduced focus on exploring temporal tracking \url{https://www.sciencedirect.com/science/article/pii/S1361841516301657?via%3Dihub}. This approach treats each frame as a separate detection problem, neglecting the temporal aspect. Tracking by detection is beneficial in complex environments where the tracked object may frequently move in and out of view, a scenario that would require explicit handling in a temporal model with a reinitialization procedure. However, in surgical skill analysis with multiple tools, we require information about each individual tool and its use, necessitating a tracking approach that can handle multiple objects simultaneously. This is particularly important in laparoscopic surgery, where multiple tools are used simultaneously, and the surgeon's hands may occlude the tools and they may move in and out of view.

% Generally, tracking by detection methods train a classifier to separate the region of interest from the background and then keep on updating with new information in each frame. The reliability of these methods may be compromised if some of the samples are incorrectly labelled. Furthermore, the substantial challenge for tracking by detection methods is that the bounding box not only contains the object of interest but rather a considerable portion of background too. This has an adverse effect on the model training since background portion keeps on hanging in different frames. https://arxiv.org/pdf/2209.01435

%  Nwoye et al (2019) uses frame-level labels to detect tool tip and track multiple instruments resulting in 12.6% improvement in SOTA.https://arxiv.org/pdf/2209.01435. 

Many existing models use extra markers to help with tracking. Though this is extremely effective, it is not always practical in a real-world setting. We aim to develop a tracking algorithm that does not require additional markers, which would be more practical in a real-world setting. 
% https://ieeexplore.ieee.org/abstract/document/8574022, https://openaccess.thecvf.com/content/CVPR2022/html/Su_ZebraPose_Coarse_To_Fine_Surface_Encoding_for_6DoF_Object_Pose_CVPR_2022_paper.html, https://link.springer.com/article/10.1007/s11548-017-1558-9, https://www.frontiersin.org/articles/10.3389/frobt.2021.751741/full., https://ieeexplore.ieee.org/abstract/document/6359786
% Some even use computer-aided design (CAD) models to assist in real-time 3D tracking https://link.springer.com/chapter/10.1007/978-3-319-46720-7_45.

\subsection{Potential Gaps and Insights}

% what has not been done whicih could be done
% pose research questions

% Data-related gaps: One of the important hurdles in integrating AI into surgery is the availability of sufficient annotated training data. The performance of AI-based models relies heavily upon data availability. Also, the data labeling requires expert annotators. Maier-Hein et al (2022) argues that the lack of success stories in surgery as contrasted to other medical domains, such as radiology and dermatology can be attributed to the lack of quality in the annotated datasets. For clinical usefulness, algorithms need to have faster inference times, and need to be tested on sufficiently fast hardware to enable real-time execution. https://arxiv.org/pdf/2209.01435

\subsection{Future Use Cases}

% Need to have a concise plan going forward. I have focused on image tracking and segmentation across many datasets. Going forward, I should consider what could be done, working with state-of-the-art (SOTAs) and focusing on something different to give different results. Using the papers from the literature review I have carried out, I should pick a paper, run the SOTA method and consider why it performs best or why it fails at certain things. These things are where I should look for improvements. Consider the mistakes made and how I could solve them. If there are good and bad samples, then focus on those which fail and innovate on what could be done about them. For example, there could be different sizes or occlusions or look at the domain-safe problem (solving the generalisation problem on real and fake data). If the methods do not work on real data, what is the gap which needs to be solved? Goal is to get inference down.