@article{hasan_detection_2021,
	title = {Detection, segmentation, and {3D} pose estimation of surgical tools using convolutional neural networks and algebraic geometry},
	volume = {70},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841521000402},
	doi = {10.1016/j.media.2021.101994},
	abstract = {Background and objective:Surgical tool detection, segmentation, and 3D pose estimation are crucial components in Computer-Assisted Laparoscopy (CAL). The existing frameworks have two main limitations. First, they do not integrate all three components. Integration is critical; for instance, one should not attempt computing pose if detection is negative. Second, they have highly specific requirements, such as the availability of a CAD model. We propose an integrated and generic framework whose sole requirement for the 3D pose is that the tool shaft is cylindrical. Our framework makes the most of deep learning and geometric 3D vision by combining a proposed Convolutional Neural Network (CNN) with algebraic geometry. We show two applications of our framework in CAL: tool-aware rendering in Augmented Reality (AR) and tool-based 3D measurement. Methods:We name our CNN as ART-Net (Augmented Reality Tool Network). It has a Single Input Multiple Output (SIMO) architecture with one encoder and multiple decoders to achieve detection, segmentation, and geometric primitive extraction. These primitives are the tool edge-lines, mid-line, and tip. They allow the tool’s 3D pose to be estimated by a fast algebraic procedure. The framework only proceeds if a tool is detected. The accuracy of segmentation and geometric primitive extraction is boosted by a new Full resolution feature map Generator (FrG). We extensively evaluate the proposed framework with the EndoVis and new proposed datasets. We compare the segmentation results against several variants of the Fully Convolutional Network (FCN) and U-Net. Several ablation studies are provided for detection, segmentation, and geometric primitive extraction. The proposed datasets are surgery videos of different patients. Results:In detection, ART-Net achieves 100.0\% in both average precision and accuracy. In segmentation, it achieves 81.0\% in mean Intersection over Union (mIoU) on the robotic EndoVis dataset (articulated tool), where it outperforms both FCN and U-Net, by 4.5pp and 2.9pp, respectively. It achieves 88.2\% in mIoU on the remaining datasets (non-articulated tool). In geometric primitive extraction, ART-Net achieves 2.45∘ and 2.23∘ in mean Arc Length (mAL) error for the edge-lines and mid-line, respectively, and 9.3 pixels in mean Euclidean distance error for the tool-tip. Finally, in terms of 3D pose evaluated on animal data, our framework achieves 1.87 mm, 0.70 mm, and 4.80 mm mean absolute errors on the X, Y, and Z coordinates, respectively, and 5.94∘ angular error on the shaft orientation. It achieves 2.59 mm and 1.99 mm in mean and median location error of the tool head evaluated on patient data. Conclusions:The proposed framework outperforms existing ones in detection and segmentation. Compared to separate networks, integrating the tasks in a single network preserves accuracy in detection and segmentation but substantially improves accuracy in geometric primitive extraction. Overall, our framework has similar or better accuracy in 3D pose estimation while largely improving robustness against the very challenging imaging conditions of laparoscopy. The source code of our framework and our annotated dataset will be made publicly available at https://github.com/kamruleee51/ART-Net.},
	urldate = {2024-08-10},
	journal = {Medical Image Analysis},
	author = {Hasan, Md. Kamrul and Calvet, Lilian and Rabbani, Navid and Bartoli, Adrien},
	month = may,
	year = {2021},
	keywords = {3D pose, Algebraic geometry, Augmented reality, Computer-assisted laparoscopy, Deep learning, Segmentation},
	pages = {101994},
	file = {Full Text:/Users/scsoc/Zotero/storage/2GCQ5DNM/Hasan et al. - 2021 - Detection, segmentation, and 3D pose estimation of.pdf:application/pdf;ScienceDirect Snapshot:/Users/scsoc/Zotero/storage/S5QSR5KS/S1361841521000402.html:text/html},
}

@article{collins_tripodai_2024,
	title = {{TRIPOD}+{AI} statement: updated guidance for reporting clinical prediction models that use regression or machine learning methods},
	issn = {1756-1833},
	shorttitle = {{TRIPOD}+{AI} statement},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj-2023-078378},
	doi = {10.1136/bmj-2023-078378},
	language = {en},
	urldate = {2024-08-12},
	journal = {BMJ},
	author = {Collins, Gary S and Moons, Karel G M and Dhiman, Paula and Riley, Richard D and Beam, Andrew L and Van Calster, Ben and Ghassemi, Marzyeh and Liu, Xiaoxuan and Reitsma, Johannes B and Van Smeden, Maarten and Boulesteix, Anne-Laure and Camaradou, Jennifer Catherine and Celi, Leo Anthony and Denaxas, Spiros and Denniston, Alastair K and Glocker, Ben and Golub, Robert M and Harvey, Hugh and Heinze, Georg and Hoffman, Michael M and Kengne, André Pascal and Lam, Emily and Lee, Naomi and Loder, Elizabeth W and Maier-Hein, Lena and Mateen, Bilal A and McCradden, Melissa D and Oakden-Rayner, Lauren and Ordish, Johan and Parnell, Richard and Rose, Sherri and Singh, Karandeep and Wynants, Laure and Logullo, Patricia},
	month = apr,
	year = {2024},
	pages = {e078378},
	file = {Full Text:/Users/scsoc/Zotero/storage/QPVMUGHI/Collins et al. - 2024 - TRIPOD+AI statement updated guidance for reportin.pdf:application/pdf},
}

@article{alabi_multitask_2024,
	title = {Multitask {Learning} in {Minimally} {Invasive} {Surgical} {Vision}: {A} {Review}},
	abstract = {Minimally invasive surgery (MIS) has revolutionized many procedures and led to reduced recovery time and risk of patient injury. However, MIS poses additional complexity and burden on surgical teams. Data-driven surgical vision algorithms are thought to be key building blocks in the development of future MIS systems with improved autonomy. Recent advancements in machine learning and computer vision have led to successful applications in analyzing videos obtained from MIS with the promise of alleviating challenges in MIS videos.},
	language = {en},
	journal = {Medical Image Analysis},
	author = {Alabi, Oluwatosin and Vercauteren, Tom and Shi, Miaojing},
	year = {2024},
	file = {Alabi et al. - 2024 - Multitask Learning in Minimally Invasive Surgical .pdf:/Users/scsoc/Zotero/storage/AYQ46MM2/Alabi et al. - 2024 - Multitask Learning in Minimally Invasive Surgical .pdf:application/pdf},
}


@article{bernal_comparative_2017,
	title = {Comparative {Validation} of {Polyp} {Detection} {Methods} in {Video} {Colonoscopy}: {Results} {From} the {MICCAI} 2015 {Endoscopic} {Vision} {Challenge}},
	volume = {36},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0278-0062, 1558-254X},
	shorttitle = {Comparative {Validation} of {Polyp} {Detection} {Methods} in {Video} {Colonoscopy}},
	url = {http://ieeexplore.ieee.org/document/7840040/},
	doi = {10.1109/TMI.2017.2664042},
	number = {6},
	urldate = {2024-08-17},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Bernal, Jorge and Tajkbaksh, Nima and Sanchez, Francisco Javier and Matuszewski, Bogdan J. and Chen, Hao and Yu, Lequan and Angermann, Quentin and Romain, Olivier and Rustad, Bjorn and Balasingham, Ilangko and Pogorelov, Konstantin and Choi, Sungbin and Debard, Quentin and Maier-Hein, Lena and Speidel, Stefanie and Stoyanov, Danail and Brandao, Patrick and Cordova, Henry and Sanchez-Montes, Cristina and Gurudu, Suryakanth R. and Fernandez-Esparrach, Gloria and Dray, Xavier and Liang, Jianming and Histace, Aymeric},
	month = jun,
	year = {2017},
	pages = {1231--1249},
	file = {Accepted Version:/Users/scsoc/Zotero/storage/6D559YPQ/Bernal et al. - 2017 - Comparative Validation of Polyp Detection Methods .pdf:application/pdf},
}

@article{moher_preferred_2010,
	title = {Preferred reporting items for systematic reviews and meta-analyses: {The} {PRISMA} statement},
	volume = {8},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {17439191},
	shorttitle = {Preferred reporting items for systematic reviews and meta-analyses},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1743919110000403},
	doi = {10.1016/j.ijsu.2010.02.007},
	language = {en},
	number = {5},
	urldate = {2024-08-19},
	journal = {International Journal of Surgery},
	author = {Moher, David and Liberati, Alessandro and Tetzlaff, Jennifer and Altman, Douglas G.},
	year = {2010},
	pages = {336--341},
}

@article{ali2023comprehensivesurveyrecentdeep,
      title={A comprehensive survey on recent deep learning-based methods applied to surgical data}, 
      author={Mansoor Ali and Rafael Martinez Garcia Pena and Gilberto Ochoa Ruiz and Sharib Ali},
      year={2023},
      eprint={2209.01435},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2209.01435}, 
}

@InProceedings{10.1007/978-3-030-58452-8_13,
author="Carion, Nicolas
and Massa, Francisco
and Synnaeve, Gabriel
and Usunier, Nicolas
and Kirillov, Alexander
and Zagoruyko, Sergey",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="End-to-End Object Detection with Transformers",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="213--229",
abstract="We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.",
isbn="978-3-030-58452-8"
}


@article{nwoye_cholectrack20_2023,
	title = {{CholecTrack20}: {A} {Dataset} for {Multi}-{Class} {Multiple} {Tool} {Tracking} in {Laparoscopic} {Surgery}},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	shorttitle = {{CholecTrack20}},
	url = {https://arxiv.org/abs/2312.07352},
	doi = {10.48550/ARXIV.2312.07352},
	abstract = {Tool tracking in surgical videos is vital in computer-assisted intervention for tasks like surgeon skill assessment, safety zone estimation, and human-machine collaboration during minimally invasive procedures. The lack of large-scale datasets hampers Artificial Intelligence implementation in this domain. Current datasets exhibit overly generic tracking formalization, often lacking surgical context: a deficiency that becomes evident when tools move out of the camera's scope, resulting in rigid trajectories that hinder realistic surgical representation. This paper addresses the need for a more precise and adaptable tracking formalization tailored to the intricacies of endoscopic procedures by introducing CholecTrack20, an extensive dataset meticulously annotated for multi-class multi-tool tracking across three perspectives representing the various ways of considering the temporal duration of a tool trajectory: (1) intraoperative, (2) intracorporeal, and (3) visibility within the camera's scope. The dataset comprises 20 laparoscopic videos with over 35,000 frames and 65,000 annotated tool instances with details on spatial location, category, identity, operator, phase, and surgical visual conditions. This detailed dataset caters to the evolving assistive requirements within a procedure.},
	urldate = {2024-08-20},
	publisher = {arXiv},
	author = {Nwoye, Chinedu Innocent and Elgohary, Kareem and Srinivas, Anvita and Zaid, Fauzan and Lavanchy, Joël L. and Padoy, Nicolas},
	year = {2023},
	note = {Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	annote = {Other
Surgical tool tracking dataset paper, 15 pages, 9 figures, 4 tables},
}

@article{oldfield_assessment_1971,
	title = {The assessment and analysis of handedness: {The} {Edinburgh} inventory},
	volume = {9},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00283932},
	shorttitle = {The assessment and analysis of handedness},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0028393271900674},
	doi = {10.1016/0028-3932(71)90067-4},
	language = {en},
	number = {1},
	urldate = {2024-08-20},
	journal = {Neuropsychologia},
	author = {Oldfield, R.C.},
	month = mar,
	year = {1971},
	pages = {97--113},
}

@article{lou_min-max_2022,
	title = {Min-{Max} {Similarity}: {A} {Contrastive} {Semi}-{Supervised} {Deep} {Learning} {Network} for {Surgical} {Tools} {Segmentation}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Min-{Max} {Similarity}},
	url = {https://arxiv.org/abs/2203.15177},
	doi = {10.48550/ARXIV.2203.15177},
	abstract = {A common problem with segmentation of medical images using neural networks is the difficulty to obtain a significant number of pixel-level annotated data for training. To address this issue, we proposed a semi-supervised segmentation network based on contrastive learning. In contrast to the previous state-of-the-art, we introduce Min-Max Similarity (MMS), a contrastive learning form of dual-view training by employing classifiers and projectors to build all-negative, and positive and negative feature pairs, respectively, to formulate the learning as solving a MMS problem. The all-negative pairs are used to supervise the networks learning from different views and to capture general features, and the consistency of unlabeled predictions is measured by pixel-wise contrastive loss between positive and negative pairs. To quantitatively and qualitatively evaluate our proposed method, we test it on four public endoscopy surgical tool segmentation datasets and one cochlear implant surgery dataset, which we manually annotated. Results indicate that our proposed method consistently outperforms state-of-the-art semi-supervised and fully supervised segmentation algorithms. And our semi-supervised segmentation algorithm can successfully recognize unknown surgical tools and provide good predictions. Also, our MMS approach could achieve inference speeds of about 40 frames per second (fps) and is suitable to deal with the real-time video segmentation.},
	urldate = {2024-08-20},
	publisher = {arXiv},
	author = {Lou, Ange and Tawfik, Kareem and Yao, Xing and Liu, Ziteng and Noble, Jack},
	year = {2022},
	note = {Version Number: 4},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Image and Video Processing (eess.IV)},
}

@article{fathabadi_box-trainer_2022,
	title = {Box-{Trainer} {Assessment} {System} with {Real}-{Time} {Multi}-{Class} {Detection} and {Tracking} of {Laparoscopic} {Instruments}, using {CNN}},
	volume = {19},
	abstract = {In Minimally Invasive Surgery (MIS), surgeons need to acquire a specific set of skills, before carrying out a “real” operation. Training with the Laparoscopic Surgical BoxTrainer device helps in acquiring the needed skills for surgery residents which are traditionally not taught to them. Video recording of residents’ performance and computerassisted surgical trainers for MIS provide valuable information for resident’s assessment. In this paper, we propose real-time detection and tracking of a multi-class of laparoscopic instruments for an intelligent box-trainer performance assessment system using SSDResNet50 V1 FPN architecture in TensorFlow backend. The dataset has been extracted from various laparoscopic box training videos. Using distance measurements and evaluation criteria constraints, we present an evaluation of the surgeon’s performance. Based on the experimental result, the trained model could identify each instrument at the score of 90\% fidelity, in each location, within a region of interest. This research is a result of a partnership between the Department of Electrical and Computer Engineering and the Department of Surgery, of the Homer Stryker M.D. School of Medicine, at Western Michigan University.},
	language = {en},
	number = {2},
	journal = {Acta Polytechnica Hungarica},
	author = {Fathabadi, Fatemeh Rashidi and Grantner, Janos L},
	year = {2022},
	file = {Fathabadi and Grantner - 2022 - Box-Trainer Assessment System with Real-Time Multi.pdf:/Users/scsoc/Zotero/storage/CD6YJCRH/Fathabadi and Grantner - 2022 - Box-Trainer Assessment System with Real-Time Multi.pdf:application/pdf},
}

@inproceedings{jin_tool_2018,
	address = {Lake Tahoe, NV},
	title = {Tool {Detection} and {Operative} {Skill} {Assessment} in {Surgical} {Videos} {Using} {Region}-{Based} {Convolutional} {Neural} {Networks}},
	isbn = {978-1-5386-4886-5},
	url = {https://ieeexplore.ieee.org/document/8354185/},
	doi = {10.1109/WACV.2018.00081},
	urldate = {2024-08-21},
	booktitle = {2018 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Jin, Amy and Yeung, Serena and Jopling, Jeffrey and Krause, Jonathan and Azagury, Dan and Milstein, Arnold and Fei-Fei, Li},
	month = mar,
	year = {2018},
	pages = {691--699},
	file = {Submitted Version:/Users/scsoc/Zotero/storage/24DSBGM8/Jin et al. - 2018 - Tool Detection and Operative Skill Assessment in S.pdf:application/pdf},
}


@article{constable_enhancing_2024,
	title = {Enhancing surgical performance in cardiothoracic surgery with innovations from computer vision and artificial intelligence: a narrative review},
	volume = {19},
	issn = {1749-8090},
	shorttitle = {Enhancing surgical performance in cardiothoracic surgery with innovations from computer vision and artificial intelligence},
	url = {https://doi.org/10.1186/s13019-024-02558-5},
	doi = {10.1186/s13019-024-02558-5},
	abstract = {When technical requirements are high, and patient outcomes are critical, opportunities for monitoring and improving surgical skills via objective motion analysis feedback may be particularly beneficial. This narrative review synthesises work on technical and non-technical surgical skills, collaborative task performance, and pose estimation to illustrate new opportunities to advance cardiothoracic surgical performance with innovations from computer vision and artificial intelligence. These technological innovations are critically evaluated in terms of the benefits they could offer the cardiothoracic surgical community, and any barriers to the uptake of the technology are elaborated upon. Like some other specialities, cardiothoracic surgery has relatively few opportunities to benefit from tools with data capture technology embedded within them (as is possible with robotic-assisted laparoscopic surgery, for example). In such cases, pose estimation techniques that allow for movement tracking across a conventional operating field without using specialist equipment or markers offer considerable potential. With video data from either simulated or real surgical procedures, these tools can (1) provide insight into the development of expertise and surgical performance over a surgeon’s career, (2) provide feedback to trainee surgeons regarding areas for improvement, (3) provide the opportunity to investigate what aspects of skill may be linked to patient outcomes which can (4) inform the aspects of surgical skill which should be focused on within training or mentoring programmes. Classifier or assessment algorithms that use artificial intelligence to ‘learn’ what expertise is from expert surgical evaluators could further assist educators in determining if trainees meet competency thresholds. With collaborative efforts between surgical teams, medical institutions, computer scientists and researchers to ensure this technology is developed with usability and ethics in mind, the developed feedback tools could improve cardiothoracic surgical practice in a data-driven way.},
	number = {1},
	urldate = {2024-08-21},
	journal = {Journal of Cardiothoracic Surgery},
	author = {Constable, Merryn D. and Shum, Hubert P. H. and Clark, Stephen},
	month = feb,
	year = {2024},
	keywords = {Deep learning, Markerless motion tracking, Pose estimation, Psychomotor ability, Surgical education, Surgical expertise, Surgical kinematics, Surgical performance, Surgical skills, Surgical training},
	pages = {94},
	file = {Full Text PDF:/Users/scsoc/Zotero/storage/ZBQBKGUK/Constable et al. - 2024 - Enhancing surgical performance in cardiothoracic s.pdf:application/pdf;Snapshot:/Users/scsoc/Zotero/storage/MVXLNC7N/s13019-024-02558-5.html:text/html},
}



@article{jaffray_minimally_2005,
	title = {Minimally invasive surgery},
	volume = {90},
	issn = {0003-9888, 1468-2044},
	url = {https://adc.bmj.com/lookup/doi/10.1136/adc.2004.062760},
	doi = {10.1136/adc.2004.062760},
	language = {en},
	number = {5},
	urldate = {2024-08-22},
	journal = {Archives of Disease in Childhood},
	author = {Jaffray, B},
	month = may,
	year = {2005},
	pages = {537--542},
	file = {Full Text:/Users/scsoc/Zotero/storage/ZC25V6XF/Jaffray - 2005 - Minimally invasive surgery.pdf:application/pdf},
}

@article{monnet_laparoscopy_2003,
	title = {Laparoscopy},
	volume = {33},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {01955616},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0195561603000585},
	doi = {10.1016/S0195-5616(03)00058-5},
	language = {en},
	number = {5},
	urldate = {2024-08-22},
	journal = {Veterinary Clinics of North America: Small Animal Practice},
	author = {Monnet, Eric and Twedt, David C},
	month = sep,
	year = {2003},
	pages = {1147--1163},
}

@article{rockall_laparoscopy_2014,
	title = {Laparoscopy in the era of enhanced recovery},
	volume = {28},
	issn = {15216918},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S152169181300228X},
	doi = {10.1016/j.bpg.2013.11.001},
	language = {en},
	number = {1},
	urldate = {2024-08-22},
	journal = {Best Practice \& Research Clinical Gastroenterology},
	author = {Rockall, T.A. and Demartines, N.},
	month = feb,
	year = {2014},
	pages = {133--142},
}

@article{meara_global_2015,
	title = {Global {Surgery} 2030: evidence and solutions for achieving health, welfare, and economic development},
	volume = {386},
	issn = {01406736},
	shorttitle = {Global {Surgery} 2030},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S014067361560160X},
	doi = {10.1016/S0140-6736(15)60160-X},
	language = {en},
	number = {9993},
	urldate = {2024-08-22},
	journal = {The Lancet},
	author = {Meara, John G and Leather, Andrew J M and Hagander, Lars and Alkire, Blake C and Alonso, Nivaldo and Ameh, Emmanuel A and Bickler, Stephen W and Conteh, Lesong and Dare, Anna J and Davies, Justine and Mérisier, Eunice Dérivois and El-Halabi, Shenaaz and Farmer, Paul E and Gawande, Atul and Gillies, Rowan and Greenberg, Sarah L M and Grimes, Caris E and Gruen, Russell L and Ismail, Edna Adan and Kamara, Thaim Buya and Lavy, Chris and Lundeg, Ganbold and Mkandawire, Nyengo C and Raykar, Nakul P and Riesel, Johanna N and Rodas, Edgar and Rose, John and Roy, Nobhojit and Shrime, Mark G and Sullivan, Richard and Verguet, Stéphane and Watters, David and Weiser, Thomas G and Wilson, Iain H and Yamey, Gavin and Yip, Winnie},
	month = aug,
	year = {2015},
	pages = {569--624},
	file = {Full Text:/Users/scsoc/Zotero/storage/YBHW8KUK/Meara et al. - 2015 - Global Surgery 2030 evidence and solutions for ac.pdf:application/pdf},
}


@book{organization_health_2016,
	title = {Health workforce requirements for universal health coverage and the {Sustainable} {Development} {Goals}. ({Human} {Resources} for {Health} {Observer}, 17)},
	isbn = {978-92-4-151140-7},
	url = {https://iris.who.int/handle/10665/250330},
	abstract = {40 p.},
	language = {en},
	urldate = {2024-08-22},
	publisher = {World Health Organization},
	author = {Organization, World Health},
	year = {2016},
	note = {Accepted: 2016-10-10T10:31:06Z},
	file = {Full Text PDF:/Users/scsoc/Zotero/storage/VPRJLRKK/Organization - 2016 - Health workforce requirements for universal health.pdf:application/pdf},
}

@article{maier-hein_surgical_2022,
	title = {Surgical data science – from concepts toward clinical translation},
	volume = {76},
	issn = {13618415},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841521003510},
	doi = {10.1016/j.media.2021.102306},
	language = {en},
	urldate = {2024-08-22},
	journal = {Medical Image Analysis},
	author = {Maier-Hein, Lena and Eisenmann, Matthias and Sarikaya, Duygu and März, Keno and Collins, Toby and Malpani, Anand and Fallert, Johannes and Feussner, Hubertus and Giannarou, Stamatia and Mascagni, Pietro and Nakawala, Hirenkumar and Park, Adrian and Pugh, Carla and Stoyanov, Danail and Vedula, Swaroop S. and Cleary, Kevin and Fichtinger, Gabor and Forestier, Germain and Gibaud, Bernard and Grantcharov, Teodor and Hashizume, Makoto and Heckmann-Nötzel, Doreen and Kenngott, Hannes G. and Kikinis, Ron and Mündermann, Lars and Navab, Nassir and Onogur, Sinan and Roß, Tobias and Sznitman, Raphael and Taylor, Russell H. and Tizabi, Minu D. and Wagner, Martin and Hager, Gregory D. and Neumuth, Thomas and Padoy, Nicolas and Collins, Justin and Gockel, Ines and Goedeke, Jan and Hashimoto, Daniel A. and Joyeux, Luc and Lam, Kyle and Leff, Daniel R. and Madani, Amin and Marcus, Hani J. and Meireles, Ozanan and Seitel, Alexander and Teber, Dogu and Ückert, Frank and Müller-Stich, Beat P. and Jannin, Pierre and Speidel, Stefanie},
	month = feb,
	year = {2022},
	pages = {102306},
	file = {Full Text:/Users/scsoc/Zotero/storage/FQ6ASMMY/Maier-Hein et al. - 2022 - Surgical data science – from concepts toward clini.pdf:application/pdf},
}

@book{world_health_organization_world_2016,
	address = {Geneva},
	title = {World health statistics 2016: monitoring health for the {SDGs}, sustainable development goals},
	isbn = {978-92-4-156526-4},
	shorttitle = {World health statistics 2016},
	url = {https://iris.who.int/handle/10665/206498},
	language = {en},
	urldate = {2024-08-22},
	publisher = {World Health Organization},
	author = {{World Health Organization}},
	year = {2016},
	keywords = {Global Health, Health Priorities, Health Status Indicators, Life Expectancy, Mortality, Organizational Objectives, Statistics, Universal Health Insurance},
	file = {World Health Organization - 2016 - World health statistics 2016 monitoring health fo.pdf:/Users/scsoc/Zotero/storage/DX58HYK4/World Health Organization - 2016 - World health statistics 2016 monitoring health fo.pdf:application/pdf},
}

@article{vassiliou_global_2005,
	title = {A global assessment tool for evaluation of intraoperative laparoscopic skills},
	volume = {190},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00029610},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0002961005003478},
	doi = {10.1016/j.amjsurg.2005.04.004},
	language = {en},
	number = {1},
	urldate = {2024-08-22},
	journal = {The American Journal of Surgery},
	author = {Vassiliou, Melina C. and Feldman, Liane S. and Andrew, Christopher G. and Bergman, Simon and Leffondré, Karen and Stanbridge, Donna and Fried, Gerald M.},
	month = jul,
	year = {2005},
	pages = {107--113},
}

@article{jones_analysis_2018,
	title = {Analysis of {Mechanical} {Forces} {Used} {During} {Laparoscopic} {Training} {Procedures}},
	volume = {32},
	copyright = {http://www.liebertpub.com/nv/resources-tools/text-and-data-mining-policy/121/},
	issn = {0892-7790, 1557-900X},
	url = {http://www.liebertpub.com/doi/10.1089/end.2017.0894},
	doi = {10.1089/end.2017.0894},
	language = {en},
	number = {6},
	urldate = {2024-08-22},
	journal = {Journal of Endourology},
	author = {Jones, Dominic and Jaffer, Ata and Nodeh, Ali Alazmani and Biyani, Chandra Shekhar and Culmer, Peter},
	month = jun,
	year = {2018},
	pages = {529--533},
	file = {Accepted Version:/Users/scsoc/Zotero/storage/2ILEXCUP/Jones et al. - 2018 - Analysis of Mechanical Forces Used During Laparosc.pdf:application/pdf},
}

@article{retrosi_motion_2015,
	title = {Motion {Analysis}–{Based} {Skills} {Training} and {Assessment} in {Pediatric} {Laparoscopy}: {Construct}, {Concurrent}, and {Content} {Validity} for the {eoSim} {Simulator}},
	volume = {25},
	copyright = {http://www.liebertpub.com/nv/resources-tools/text-and-data-mining-policy/121/},
	issn = {1092-6429, 1557-9034},
	shorttitle = {Motion {Analysis}–{Based} {Skills} {Training} and {Assessment} in {Pediatric} {Laparoscopy}},
	url = {http://www.liebertpub.com/doi/10.1089/lap.2015.0069},
	doi = {10.1089/lap.2015.0069},
	language = {en},
	number = {11},
	urldate = {2024-08-22},
	journal = {Journal of Laparoendoscopic \& Advanced Surgical Techniques},
	author = {Retrosi, Giuseppe and Cundy, Thomas and Haddad, Munther and Clarke, Simon},
	month = nov,
	year = {2015},
	pages = {944--950},
}

@article{levin_automated_2019,
	title = {Automated {Methods} of {Technical} {Skill} {Assessment} in {Surgery}: {A} {Systematic} {Review}},
	volume = {76},
	issn = {19317204},
	shorttitle = {Automated {Methods} of {Technical} {Skill} {Assessment} in {Surgery}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1931720419301643},
	doi = {10.1016/j.jsurg.2019.06.011},
	language = {en},
	number = {6},
	urldate = {2024-08-22},
	journal = {Journal of Surgical Education},
	author = {Levin, Marc and McKechnie, Tyler and Khalid, Shuja and Grantcharov, Teodor P. and Goldenberg, Mitchell},
	month = nov,
	year = {2019},
	pages = {1629--1639},
}

@article{paley_crowdsourced_2021,
	title = {Crowdsourced {Assessment} of {Surgical} {Skill} {Proficiency} in {Cataract} {Surgery}},
	volume = {78},
	issn = {19317204},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1931720421000477},
	doi = {10.1016/j.jsurg.2021.02.004},
	language = {en},
	number = {4},
	urldate = {2024-08-22},
	journal = {Journal of Surgical Education},
	author = {Paley, Grace L. and Grove, Rebecca and Sekhar, Tejas C. and Pruett, Jack and Stock, Michael V. and Pira, Tony N. and Shields, Steven M. and Waxman, Evan L. and Wilson, Bradley S. and Gordon, Mae O. and Culican, Susan M.},
	month = jul,
	year = {2021},
	pages = {1077--1088},
	file = {Full Text:/Users/scsoc/Zotero/storage/3434UHBP/Paley et al. - 2021 - Crowdsourced Assessment of Surgical Skill Proficie.pdf:application/pdf},
}

@article{allan_toward_2013,
	title = {Toward {Detection} and {Localization} of {Instruments} in {Minimally} {Invasive} {Surgery}},
	volume = {60},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0018-9294, 1558-2531},
	url = {http://ieeexplore.ieee.org/document/6359786/},
	doi = {10.1109/TBME.2012.2229278},
	number = {4},
	urldate = {2024-08-22},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {Allan, M. and Ourselin, S. and Thompson, S. and Hawkes, D. J. and Kelly, J. and Stoyanov, D.},
	month = apr,
	year = {2013},
	pages = {1050--1058},
}

@article{bodenstedt_comparative_2018,
	title = {Comparative evaluation of instrument segmentation and tracking methods in minimally invasive surgery},
	url = {http://arxiv.org/abs/1805.02475},
	abstract = {Intraoperative segmentation and tracking of minimally invasive instruments is a prerequisite for computer- and robotic-assisted surgery. Since additional hardware like tracking systems or the robot encoders are cumbersome and lack accuracy, surgical vision is evolving as promising techniques to segment and track the instruments using only the endoscopic images. However, what is missing so far are common image data sets for consistent evaluation and benchmarking of algorithms against each other. The paper presents a comparative validation study of different vision-based methods for instrument segmentation and tracking in the context of robotic as well as conventional laparoscopic surgery. The contribution of the paper is twofold: we introduce a comprehensive validation data set that was provided to the study participants and present the results of the comparative validation study. Based on the results of the validation study, we arrive at the conclusion that modern deep learning approaches outperform other methods in instrument segmentation tasks, but the results are still not perfect. Furthermore, we show that merging results from different methods actually significantly increases accuracy in comparison to the best stand-alone method. On the other hand, the results of the instrument tracking task show that this is still an open challenge, especially during challenging scenarios in conventional laparoscopic surgery.},
	urldate = {2024-08-22},
	publisher = {arXiv},
	author = {Bodenstedt, Sebastian and Allan, Max and Agustinos, Anthony and Du, Xiaofei and Garcia-Peraza-Herrera, Luis and Kenngott, Hannes and Kurmann, Thomas and Müller-Stich, Beat and Ourselin, Sebastien and Pakhomov, Daniil and Sznitman, Raphael and Teichmann, Marvin and Thoma, Martin and Vercauteren, Tom and Voros, Sandrine and Wagner, Martin and Wochner, Pamela and Maier-Hein, Lena and Stoyanov, Danail and Speidel, Stefanie},
	month = may,
	year = {2018},
	note = {arXiv:1805.02475 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/scsoc/Zotero/storage/TFPU57DQ/Bodenstedt et al. - 2018 - Comparative evaluation of instrument segmentation .pdf:application/pdf;arXiv.org Snapshot:/Users/scsoc/Zotero/storage/Z6F6HZY7/1805.html:text/html},
}

@article{loza_realtime_2024,
	title = {Real‐time surgical tool detection with multi‐scale positional encoding and contrastive learning},
	volume = {11},
	issn = {2053-3713, 2053-3713},
	url = {https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/htl2.12060},
	doi = {10.1049/htl2.12060},
	abstract = {Abstract
            Real‐time detection of surgical tools in laparoscopic data plays a vital role in understanding surgical procedures, evaluating the performance of trainees, facilitating learning, and ultimately supporting the autonomy of robotic systems. Existing detection methods for surgical data need to improve processing speed and high prediction accuracy. Most methods rely on anchors or region proposals, limiting their adaptability to variations in tool appearance and leading to sub‐optimal detection results. Moreover, using non‐anchor‐based detectors to alleviate this problem has been partially explored without remarkable results. An anchor‐free architecture based on a transformer that allows real‐time tool detection is introduced. The proposal is to utilize multi‐scale features within the feature extraction layer and at the transformer‐based detection architecture through positional encoding that can refine and capture context‐aware and structural information of different‐sized tools. Furthermore, a supervised contrastive loss is introduced to optimize representations of object embeddings, resulting in improved feed‐forward network performances for classifying localized bounding boxes. The strategy demonstrates superiority to state‐of‐the‐art (SOTA) methods. Compared to the most accurate existing SOTA (DSSS) method, the approach has an improvement of nearly 4\% on mAP and a reduction in the inference time by 113\%. It also showed a 7\% higher mAP than the baseline model.},
	language = {en},
	number = {2-3},
	urldate = {2024-08-22},
	journal = {Healthcare Technology Letters},
	author = {Loza, Gerardo and Valdastri, Pietro and Ali, Sharib},
	month = apr,
	year = {2024},
	pages = {48--58},
}

@article{bouget_vision-based_2017,
	title = {Vision-based and marker-less surgical tool detection and tracking: a review of the literature},
	volume = {35},
	issn = {13618415},
	shorttitle = {Vision-based and marker-less surgical tool detection and tracking},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841516301657},
	doi = {10.1016/j.media.2016.09.003},
	language = {en},
	urldate = {2024-08-22},
	journal = {Medical Image Analysis},
	author = {Bouget, David and Allan, Max and Stoyanov, Danail and Jannin, Pierre},
	month = jan,
	year = {2017},
	pages = {633--654},
	file = {Submitted Version:/Users/scsoc/Zotero/storage/KQWNXPFJ/Bouget et al. - 2017 - Vision-based and marker-less surgical tool detecti.pdf:application/pdf},
}
@incollection{koonce_resnet_2021,
	address = {Berkeley, CA},
	title = {{ResNet} 50},
	isbn = {978-1-4842-6167-5 978-1-4842-6168-2},
	url = {http://link.springer.com/10.1007/978-1-4842-6168-2_6},
	language = {en},
	urldate = {2024-08-23},
	booktitle = {Convolutional {Neural} {Networks} with {Swift} for {Tensorflow}},
	publisher = {Apress},
	author = {Koonce, Brett},
	collaborator = {Koonce, Brett},
	year = {2021},
	doi = {10.1007/978-1-4842-6168-2_6},
	pages = {63--72},
}

@article{teevno_semi-supervised_2023,
	title = {A semi-supervised {Teacher}-{Student} framework for surgical tool detection and localization},
	volume = {11},
	issn = {2168-1163, 2168-1171},
	url = {https://www.tandfonline.com/doi/full/10.1080/21681163.2022.2150688},
	doi = {10.1080/21681163.2022.2150688},
	language = {en},
	number = {4},
	urldate = {2024-08-24},
	journal = {Computer Methods in Biomechanics and Biomedical Engineering: Imaging \& Visualization},
	author = {Teevno, Mansoor Ali and Ochoa-Ruiz, Gilberto and Ali, Sharib},
	month = jul,
	year = {2023},
	pages = {1033--1041},
}

@article{martin_objective_1997,
	title = {Objective structured assessment of technical skill ({OSATS}) for surgical residents: {OBJECTIVE} {STRUCTURED} {ASSESSMENT} {OF} {TECHNICAL} {SKILL}},
	volume = {84},
	copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
	issn = {00071323},
	shorttitle = {Objective structured assessment of technical skill ({OSATS}) for surgical residents},
	url = {https://academic.oup.com/bjs/article/84/2/273/6167081},
	doi = {10.1046/j.1365-2168.1997.02502.x},
	language = {en},
	number = {2},
	urldate = {2024-08-24},
	journal = {British Journal of Surgery},
	author = {Martin, J. A. and Regehr, G. and Reznick, R. and Macrae, H. and Murnaghan, J. and Hutchison, C. and Brown, M.},
	month = feb,
	year = {1997},
	pages = {273--278},
}

@article{hussein_development_2017,
	title = {Development and {Validation} of an {Objective} {Scoring} {Tool} for {Robot}-{Assisted} {Radical} {Prostatectomy}: {Prostatectomy} {Assessment} and {Competency} {Evaluation}},
	volume = {197},
	issn = {0022-5347, 1527-3792},
	shorttitle = {Development and {Validation} of an {Objective} {Scoring} {Tool} for {Robot}-{Assisted} {Radical} {Prostatectomy}},
	url = {http://www.jurology.com/doi/10.1016/j.juro.2016.11.100},
	doi = {10.1016/j.juro.2016.11.100},
	language = {en},
	number = {5},
	urldate = {2024-08-24},
	journal = {Journal of Urology},
	author = {Hussein, Ahmed A. and Ghani, Khurshid R. and Peabody, James and Sarle, Richard and Abaza, Ronney and Eun, Daniel and Hu, Jim and Fumo, Michael and Lane, Brian and Montgomery, Jeffrey S. and Hinata, Nobuyuki and Rooney, Deborah and Comstock, Bryan and Chan, Hei Kit and Mane, Sridhar S. and Mohler, James L. and Wilding, Gregory and Miller, David and Guru, Khurshid A. and {Michigan Urological Surgery Improvement Collaborative and Applied Technology Laboratory for Advanced Surgery Program}},
	month = may,
	year = {2017},
	pages = {1237--1244},
}

@article{hilal_randomized_2017,
	title = {A randomized comparison of video demonstration versus hands-on training of medical students for vacuum delivery using {Objective} {Structured} {Assessment} of {Technical} {Skills} ({OSATS})},
	volume = {96},
	issn = {0025-7974},
	url = {https://journals.lww.com/00005792-201703170-00052},
	doi = {10.1097/MD.0000000000006355},
	language = {en},
	number = {11},
	urldate = {2024-08-24},
	journal = {Medicine},
	author = {Hilal, Ziad and Kumpernatz, Anne K. and Rezniczek, Günther A. and Cetin, Cem and Tempfer-Bentz, Eva-Katrin and Tempfer, Clemens B.},
	month = mar,
	year = {2017},
	pages = {e6355},
	file = {Full Text:/Users/scsoc/Zotero/storage/RKX45GPC/Hilal et al. - 2017 - A randomized comparison of video demonstration ver.pdf:application/pdf},
}

@article{pears_capturing_2021,
	title = {Capturing the non-technical skills of a technical skills trainer ({NTS}-{TeST}) during simulation},
	volume = {66},
	issn = {0036-9330, 2045-6441},
	url = {http://journals.sagepub.com/doi/10.1177/00369330211008594},
	doi = {10.1177/00369330211008594},
	abstract = {Objective
              To develop an assessment instrument that can be used as a comprehensive feedback record to convey to a trainer the non-technical aspects of skill acquisition and training.
            
            
              Methods
              The instrument was developed across three rounds. In Round 1, 6 endourological consultants undertook a modified Delphi process. Round 2 included 10 trainers who assessed each question’s relevance and practicability. Round 3 involved a pilot study with fifteen urology residents who participated in a technical skills simulation session with the incorporation of the instrument. We report the content, face, and construct validity, and the internal consistency of an NTS instrument for trainers.
            
            
              Results
              The instrument had a consistent and a high positive average for each of the 4 sections of the instrument, regardless of the type of user. Positive Spearman’s correlation coefficients (0.02 to .64) for content validity and Cronbach’s alpha (a = 0.70) indicated good validity and moderate reliability of the instrument.
            
            
              Conclusion
              We propose a novel NTS instrument for trainers during a simulation. This instrument can be used for benchmarking the quality of technical skills simulation training.},
	language = {en},
	number = {3},
	urldate = {2024-08-24},
	journal = {Scottish Medical Journal},
	author = {Pears, Matthew and Biyani, Chandra Shekhar and Joyce, Adrian D and Spearpoint, Ken and Yiasemidou, Marina and Cleynenbreugel, Ben Van and Patterson, Jake and Mushtaq, Faisal},
	month = aug,
	year = {2021},
	pages = {124--133},
}

@misc{ultralytics_yolov10_2024,
	title = {{YOLOv10} - {Ultralytics} {YOLO} {Docs}},
	url = {https://docs.ultralytics.com/models/yolov10/},
	urldate = {2024-08-25},
	file = {YOLOv10 - Ultralytics YOLO Docs:/Users/scsoc/Zotero/storage/75BLN7M7/yolov10.html:text/html},
	author = {Ultralytics},
	year = {2024},
}

@article{zlocha_improving_2019,
	title = {Improving {RetinaNet} for {CT} {Lesion} {Detection} with {Dense} {Masks} from {Weak} {RECIST} {Labels}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1906.02283},
	doi = {10.48550/ARXIV.1906.02283},
	abstract = {Accurate, automated lesion detection in Computed Tomography (CT) is an important yet challenging task due to the large variation of lesion types, sizes, locations and appearances. Recent work on CT lesion detection employs two-stage region proposal based methods trained with centroid or bounding-box annotations. We propose a highly accurate and efficient one-stage lesion detector, by re-designing a RetinaNet to meet the particular challenges in medical imaging. Specifically, we optimize the anchor configurations using a differential evolution search algorithm. For training, we leverage the response evaluation criteria in solid tumors (RECIST) annotation which are measured in clinical routine. We incorporate dense masks from weak RECIST labels, obtained automatically using GrabCut, into the training objective, which in combination with other advancements yields new state-of-the-art performance. We evaluate our method on the public DeepLesion benchmark, consisting of 32,735 lesions across the body. Our one-stage detector achieves a sensitivity of 90.77\% at 4 false positives per image, significantly outperforming the best reported methods by over 5\%.},
	urldate = {2024-08-25},
	publisher = {arXiv},
	author = {Zlocha, Martin and Dou, Qi and Glocker, Ben},
	year = {2019},
	note = {Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Image and Video Processing (eess.IV), Machine Learning (cs.LG)},
	annote = {Other
Accepted at MICCAI 2019},
}

@article{Rashidi_Fathabadi_Grantner_Shebrain_Abdel,
	title = {Autonomous sequential surgical skills assessment for the peg transfer task in a laparoscopic box-trainer system with three cameras},
	volume = {41},
	DOI = {10.1017/S0263574723000218},
	number = {6},
	journal = {Robotica},
	author = {Rashidi Fathabadi, Fatemeh and Grantner, Janos L. and Shebrain, Saad A and Abdel-Qader, Ikhlas},
	year = {2023},
	pages = {1837–1855},
}


@article{maciel_development_2008,
	title = {Development of the {VBLaST} $^{\textrm{™}}$ : a virtual basic laparoscopic skill trainer},
	volume = {4},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	issn = {1478-5951, 1478-596X},
	shorttitle = {Development of the {VBLaST} $^{\textrm{™}}$},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/rcs.185},
	doi = {10.1002/rcs.185},
	abstract = {Abstract
            
              Background
              The FLS training tool box has now been adopted by the Society of Gastrointestinal Endoscopic Surgeons (SAGES) as an official training tool for minimally invasive procedures.
            
            
              Methods
              To overcome the limitations of the physical FLS training tool box, we have developed a Virtual Basic Laparoscopic Skill Trainer (VBLaSTTM) system, which is a 3D simulator that will allow trainees to acquire basic laparoscopic skill.
            
            
              Results
              The outcome of this work is the development of an integrated visio‐haptic workstation environment including force feedback devices and a stereo display interface whereby trainees can practice on virtual versions of the FLS. Realistic graphical rendering and high fidelity haptic interactions are achieved.
            
            
              Conclusions
              Surgical skill training is a long and tedious process of acquiring fine motor skills. It is expected that residents would start on trainers such as VBLaSTTM and after reaching a certain level of competence would progress to the more complex trainers for training on specific surgical procedures. Copyright © 2008 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {2},
	urldate = {2024-08-26},
	journal = {The International Journal of Medical Robotics and Computer Assisted Surgery},
	author = {Maciel, Anderson and Liu, Youquan and Ahn, Woojin and Singh, T. Paul and Dunnican, Ward and De, Suvranu},
	month = jun,
	year = {2008},
	pages = {131--138},
	file = {Accepted Version:/Users/scsoc/Zotero/storage/45FF3LU3/Maciel et al. - 2008 - Development of the VBLaST ™  a virtual.pdf:application/pdf},
}

@article{matsumoto_laparoscopic_2022,
	title = {Laparoscopic surgical skill evaluation with motion capture and eyeglass gaze cameras: {A} pilot study},
	volume = {15},
	issn = {1758-5902, 1758-5910},
	shorttitle = {Laparoscopic surgical skill evaluation with motion capture and eyeglass gaze cameras},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/ases.13065},
	doi = {10.1111/ases.13065},
	abstract = {Abstract
            
              Introduction
              An eyeglass gaze camera and a skeletal coordinate camera without sensors attached to the operator's body were used to monitor gaze and movement during a simulated surgical procedure. These new devices have the potential to change skill assessment for laparoscopic surgery. The suitability of these devices for skill assessment was investigated.
            
            
              Material and Methods
              Six medical students, six intermediate surgeons, and four experts performed suturing tasks in a dry box. The tip positions of the instruments were identified from video recordings. Performance was evaluated based on instrument movement, gaze, and skeletal coordination.
            
            
              Results
              Task performance time and skeletal coordinates were not significantly different among skill levels. The total movement distance of the right instrument was significantly different depending on the skill level. The SD of the gaze coordinates was significantly different depending on skill level and was less for experts. The expert's gaze stayed in a small area with little blurring.
            
            
              Conclusions
              The SD of gaze point coordinates correlates with laparoscopic surgical skill level. These devices may facilitate objective intraoperative skill evaluation in future studies.},
	language = {en},
	number = {3},
	urldate = {2024-08-26},
	journal = {Asian Journal of Endoscopic Surgery},
	author = {Matsumoto, Shiro and Kawahira, Hiroshi and Oiwa, Kosuke and Maeda, Yoshitaka and Nozawa, Akio and Lefor, Alan Kawarai and Hosoya, Yoshinori and Sata, Naohiro},
	month = jul,
	year = {2022},
	pages = {619--628},
}


@misc{urology2023,
	title = {Medical {Education} {Leeds} - {Course}: {UBOOT} 22},
	url = {https://www.maxcourse.co.uk/medicaleducationleeds/guestCourseDetails.asp?cKey=1443},
	urldate = {2024-08-26},
	file = {Medical Education Leeds - Course\: UBOOT 22:/Users/scsoc/Zotero/storage/25RNHZMC/guestCourseDetails.html:text/html},
	year = {2023},
	author = {Medical Education Leeds},
}

@misc{herrera_luiscarlosgphkeypoint-annotation-tool_2024,
	title = {luiscarlosgph/keypoint-annotation-tool},
	copyright = {MIT},
	url = {https://github.com/luiscarlosgph/keypoint-annotation-tool},
	abstract = {Web app to annotate images for surgical tooltip localisation.},
	urldate = {2024-08-26},
	author = {Herrera, Luis C. Garcia Peraza},
	month = jun,
	year = {2024},
}

@misc{choudhry_omarioscmsc-surgical-tool-tracking_2024,
	title = {omariosc/msc-surgical-tool-tracking},
	url = {https://github.com/omariosc/msc-surgical-tool-tracking},
	abstract = {MSc Code},
	urldate = {2024-08-26},
	author = {Choudhry, Omar},
	month = aug,
	year = {2024},
}

@misc{aliexpress,
	title = {Simulated {Insufflated} {Belly} {Laparoscopic} {Trainer} {Simulator} {Training} {Box} with 30+0 {Degree} {HD} {Camera} {And} 10 {Training} {Modules} - {AliExpress} 21},
	url = {https://www.aliexpress.com/item/1005006809230797.html},
	urldate = {2024-08-26},
	file = {Simulated Insufflated Belly Laparoscopic Trainer Simulator Training Box with 30+0 Degree HD Camera And 10 Training Modules - AliExpress 21:/Users/scsoc/Zotero/storage/4KVWRD4F/1005006809230797.html:text/html},
	author={Surgskill},
	year={2023},
}

@article{hummel_design_2005,
	title = {Design and application of an assessment protocol for electromagnetic tracking systems: {Standardized} assessment protocol},
	volume = {32},
	copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1},
	issn = {00942405},
	shorttitle = {Design and application of an assessment protocol for electromagnetic tracking systems},
	url = {http://doi.wiley.com/10.1118/1.1944327},
	doi = {10.1118/1.1944327},
	language = {en},
	number = {7Part1},
	urldate = {2024-08-26},
	journal = {Medical Physics},
	author = {Hummel, Johann B and Bax, Michael R. and Figl, Michael L. and Kang, Yan and Maurer, Calvin and Birkfellner, Wolfgang W. and Bergmann, Helmar and Shahidi, Ramin},
	month = jun,
	year = {2005},
	pages = {2371--2379},
}

@phdthesis{nwoye:tel-03855189,
  TITLE = {{Deep learning methods for the detection and recognition of surgical tools and activities in laparoscopic videos}},
  AUTHOR = {Nwoye, Chinedu Innocent},
  URL = {https://theses.hal.science/tel-03855189},
  NUMBER = {2021STRAD038},
  SCHOOL = {{Universit{\'e} de Strasbourg}},
  YEAR = {2021},
  MONTH = Nov,
  KEYWORDS = {Deep learning ; Tool detection ; Tool tracking ; Tool-tissue interaction ; Action triplet recognition ; CholecT50 ; Weak supervision ; Attention mechanism ; Apprentissage profond ; D{\'e}tection d'outils ; Suivi d'outils ; Interaction outil-tissu ; Reconnaissance de triplet d'action ; CholectT50 ; Supervision faible ; M{\'e}canisme d'attention},
  TYPE = {Theses},
  PDF = {https://theses.hal.science/tel-03855189/file/Nwoye_Chinedu_2021_ED269.pdf},
  HAL_ID = {tel-03855189},
  HAL_VERSION = {v1},
}

@article{yolo7_2024,
	title = {{YOLOv7}‐{RepFPN}: {Improving} real‐time performance of laparoscopic tool detection on embedded systems},
	volume = {11},
	issn = {2053-3713, 2053-3713},
	shorttitle = {{YOLOv7}‐{RepFPN}},
	url = {https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/htl2.12072},
	doi = {10.1049/htl2.12072},
	abstract = {Abstract
            This study focuses on enhancing the inference speed of laparoscopic tool detection on embedded devices. Laparoscopy, a minimally invasive surgery technique, markedly reduces patient recovery times and postoperative complications. Real‐time laparoscopic tool detection helps assisting laparoscopy by providing information for surgical navigation, and its implementation on embedded devices is gaining interest due to the portability, network independence and scalability of the devices. However, embedded devices often face computation resource limitations, potentially hindering inference speed. To mitigate this concern, the work introduces a two‐fold modification to the YOLOv7 model: the feature channels and integrate RepBlock is halved, yielding the YOLOv7‐RepFPN model. This configuration leads to a significant reduction in computational complexity. Additionally, the focal EIoU (efficient intersection of union) loss function is employed for bounding box regression. Experimental results on an embedded device demonstrate that for frame‐by‐frame laparoscopic tool detection, the proposed YOLOv7‐RepFPN achieved an mAP of 88.2\% (with IoU set to 0.5) on a custom dataset based on EndoVis17, and an inference speed of 62.9 FPS. Contrasting with the original YOLOv7, which garnered an 89.3\% mAP and 41.8 FPS under identical conditions, the methodology enhances the speed by 21.1 FPS while maintaining detection accuracy. This emphasizes the effectiveness of the work.},
	language = {en},
	number = {2-3},
	urldate = {2024-08-26},
	journal = {Healthcare Technology Letters},
	author = {Liu, Yuzhang and Hayashi, Yuichiro and Oda, Masahiro and Kitasaka, Takayuki and Mori, Kensaku},
	month = apr,
	year = {2024},
	pages = {157--166},
}

@article{SurgiTrack,
	title = {{SurgiTrack}: {Fine}-{Grained} {Multi}-{Class} {Multi}-{Tool} {Tracking} in {Surgical} {Videos}},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	shorttitle = {{SurgiTrack}},
	url = {https://arxiv.org/abs/2405.20333},
	doi = {10.48550/ARXIV.2405.20333},
	abstract = {Accurate tool tracking is essential for the success of computer-assisted intervention. Previous efforts often modeled tool trajectories rigidly, overlooking the dynamic nature of surgical procedures, especially tracking scenarios like out-of-body and out-of-camera views. Addressing this limitation, the new CholecTrack20 dataset provides detailed labels that account for multiple tool trajectories in three perspectives: (1) intraoperative, (2) intracorporeal, and (3) visibility, representing the different types of temporal duration of tool tracks. These fine-grained labels enhance tracking flexibility but also increase the task complexity. Re-identifying tools after occlusion or re-insertion into the body remains challenging due to high visual similarity, especially among tools of the same category. This work recognizes the critical role of the tool operators in distinguishing tool track instances, especially those belonging to the same tool category. The operators' information are however not explicitly captured in surgical videos. We therefore propose SurgiTrack, a novel deep learning method that leverages YOLOv7 for precise tool detection and employs an attention mechanism to model the originating direction of the tools, as a proxy to their operators, for tool re-identification. To handle diverse tool trajectory perspectives, SurgiTrack employs a harmonizing bipartite matching graph, minimizing conflicts and ensuring accurate tool identity association. Experimental results on CholecTrack20 demonstrate SurgiTrack's effectiveness, outperforming baselines and state-of-the-art methods with real-time inference capability. This work sets a new standard in surgical tool tracking, providing dynamic trajectories for more adaptable and precise assistance in minimally invasive surgeries.},
	urldate = {2024-08-28},
	publisher = {arXiv},
	author = {Nwoye, Chinedu Innocent and Padoy, Nicolas},
	year = {2024},
	note = {Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	annote = {Other
15 pages, 7 figures, 9 tables, 1 video. Supplementary video available at: https://vimeo.com/951853260},
}

@article{ByteTrack,
	title = {{ByteTrack}: {Multi}-{Object} {Tracking} by {Associating} {Every} {Detection} {Box}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{ByteTrack}},
	url = {https://arxiv.org/abs/2110.06864},
	doi = {10.48550/ARXIV.2110.06864},
	abstract = {Multi-object tracking (MOT) aims at estimating bounding boxes and identities of objects in videos. Most methods obtain identities by associating detection boxes whose scores are higher than a threshold. The objects with low detection scores, e.g. occluded objects, are simply thrown away, which brings non-negligible true object missing and fragmented trajectories. To solve this problem, we present a simple, effective and generic association method, tracking by associating almost every detection box instead of only the high score ones. For the low score detection boxes, we utilize their similarities with tracklets to recover true objects and filter out the background detections. When applied to 9 different state-of-the-art trackers, our method achieves consistent improvement on IDF1 score ranging from 1 to 10 points. To put forwards the state-of-the-art performance of MOT, we design a simple and strong tracker, named ByteTrack. For the first time, we achieve 80.3 MOTA, 77.3 IDF1 and 63.1 HOTA on the test set of MOT17 with 30 FPS running speed on a single V100 GPU. ByteTrack also achieves state-of-the-art performance on MOT20, HiEve and BDD100K tracking benchmarks. The source code, pre-trained models with deploy versions and tutorials of applying to other trackers are released at https://github.com/ifzhang/ByteTrack.},
	urldate = {2024-08-29},
	publisher = {arXiv},
	author = {Zhang, Yifu and Sun, Peize and Jiang, Yi and Yu, Dongdong and Weng, Fucheng and Yuan, Zehuan and Luo, Ping and Liu, Wenyu and Wang, Xinggang},
	year = {2021},
	note = {Version Number: 3},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
}

@article{SMILEtrack,
	title = {{SMILEtrack}: {SiMIlarity} {LEarning} for {Occlusion}-{Aware} {Multiple} {Object} {Tracking}},
	shorttitle = {{SMILEtrack}},
	url = {http://arxiv.org/abs/2211.08824},
	abstract = {Despite recent progress in Multiple Object Tracking (MOT), several obstacles such as occlusions, similar objects, and complex scenes remain an open challenge. Meanwhile, a systematic study of the cost-performance tradeoff for the popular tracking-by-detection paradigm is still lacking. This paper introduces SMILEtrack, an innovative object tracker that effectively addresses these challenges by integrating an efficient object detector with a Siamese network-based Similarity Learning Module (SLM). The technical contributions of SMILETrack are twofold. First, we propose an SLM that calculates the appearance similarity between two objects, overcoming the limitations of feature descriptors in Separate Detection and Embedding (SDE) models. The SLM incorporates a Patch Self-Attention (PSA) block inspired by the vision Transformer, which generates reliable features for accurate similarity matching. Second, we develop a Similarity Matching Cascade (SMC) module with a novel GATE function for robust object matching across consecutive video frames, further enhancing MOT performance. Together, these innovations help SMILETrack achieve an improved trade-off between the cost (\{{\textbackslash}em e.g.\}, running speed) and performance (e.g., tracking accuracy) over several existing state-of-the-art benchmarks, including the popular BYTETrack method. SMILETrack outperforms BYTETrack by 0.4-0.8 MOTA and 2.1-2.2 HOTA points on MOT17 and MOT20 datasets. Code is available at https://github.com/pingyang1117/SMILEtrack\_Official},
	urldate = {2024-08-29},
	publisher = {arXiv},
	author = {Wang, Yu-Hsiang and Hsieh, Jun-Wei and Chen, Ping-Yang and Chang, Ming-Ching and So, Hung Hin and Li, Xin},
	month = jan,
	year = {2024},
	note = {arXiv:2211.08824 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Our paper was accepted by AAAI2024},
	file = {arXiv Fulltext PDF:/Users/scsoc/Zotero/storage/J8LAAHAI/Wang et al. - 2024 - SMILEtrack SiMIlarity LEarning for Occlusion-Awar.pdf:application/pdf;arXiv.org Snapshot:/Users/scsoc/Zotero/storage/Q3YIEQIH/2211.html:text/html},
}

@article{nwoye_2019,
	title = {Weakly supervised convolutional {LSTM} approach for tool tracking in laparoscopic videos},
	volume = {14},
	issn = {1861-6410, 1861-6429},
	url = {http://link.springer.com/10.1007/s11548-019-01958-6},
	doi = {10.1007/s11548-019-01958-6},
	language = {en},
	number = {6},
	urldate = {2024-08-29},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Nwoye, Chinedu Innocent and Mutter, Didier and Marescaux, Jacques and Padoy, Nicolas},
	month = jun,
	year = {2019},
	pages = {1059--1067},
	file = {Submitted Version:/Users/scsoc/Zotero/storage/64XZC5JV/Nwoye et al. - 2019 - Weakly supervised convolutional LSTM approach for .pdf:application/pdf},
}

@article{BoT-SORT,
	title = {{BoT}-{SORT}: {Robust} {Associations} {Multi}-{Pedestrian} {Tracking}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{BoT}-{SORT}},
	url = {https://arxiv.org/abs/2206.14651},
	doi = {10.48550/ARXIV.2206.14651},
	abstract = {The goal of multi-object tracking (MOT) is detecting and tracking all the objects in a scene, while keeping a unique identifier for each object. In this paper, we present a new robust state-of-the-art tracker, which can combine the advantages of motion and appearance information, along with camera-motion compensation, and a more accurate Kalman filter state vector. Our new trackers BoT-SORT, and BoT-SORT-ReID rank first in the datasets of MOTChallenge [29, 11] on both MOT17 and MOT20 test sets, in terms of all the main MOT metrics: MOTA, IDF1, and HOTA. For MOT17: 80.5 MOTA, 80.2 IDF1, and 65.0 HOTA are achieved. The source code and the pre-trained models are available at https://github.com/NirAharon/BOT-SORT},
	urldate = {2024-08-29},
	publisher = {arXiv},
	author = {Aharon, Nir and Orfaig, Roy and Bobrovsky, Ben-Zion},
	year = {2022},
	note = {Version Number: 2},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
}

@incollection{ourselin_real-time_2016,
	address = {Cham},
	title = {Real-{Time} {Online} {Adaption} for {Robust} {Instrument} {Tracking} and {Pose} {Estimation}},
	volume = {9900},
	isbn = {978-3-319-46719-1 978-3-319-46720-7},
	url = {https://link.springer.com/10.1007/978-3-319-46720-7_49},
	language = {en},
	urldate = {2024-08-30},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2016},
	publisher = {Springer International Publishing},
	author = {Rieke, Nicola and Tan, David Joseph and Tombari, Federico and Vizcaíno, Josué Page and Di San Filippo, Chiara Amat and Eslami, Abouzar and Navab, Nassir},
	editor = {Ourselin, Sebastien and Joskowicz, Leo and Sabuncu, Mert R. and Unal, Gozde and Wells, William},
	year = {2016},
	doi = {10.1007/978-3-319-46720-7_49},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {422--430},
}

@article{magro_dual-instrument_2024,
	title = {A dual-instrument {Kalman}-based tracker to enhance robustness of microsurgical tools tracking},
	issn = {1861-6429},
	url = {https://link.springer.com/10.1007/s11548-024-03246-4},
	doi = {10.1007/s11548-024-03246-4},
	abstract = {Abstract
            
              
                Purpose:
              
              The integration of a surgical robotic instrument tracking module within optical microscopes holds the potential to advance microsurgery practices, as it facilitates automated camera movements, thereby augmenting the surgeon’s capability in executing surgical procedures.
            
            
              
                Methods:
              
              In the present work, an innovative detection backbone based on spatial attention module is implemented to enhance the detection accuracy of small objects within the image. Additionally, we have introduced a robust data association technique, capable to re-track surgical instrument, mainly based on the knowledge of the dual-instrument robotics system, Intersection over Union metric and Kalman filter.
            
            
              
                Results:
              
              The effectiveness of this pipeline was evaluated through testing on a dataset comprising ten manually annotated videos of anastomosis procedures involving either animal or phantom vessels, exploiting the Symani®Surgical System—a dedicated robotic platform designed for microsurgery. The multiple object tracking precision (MOTP) and the multiple object tracking accuracy (MOTA) are used to evaluate the performance of the proposed approach, and a new metric is computed to demonstrate the efficacy in stabilizing the tracking result along the video frames. An average MOTP of 74±0.06\% and a MOTA of 99±0.03\% over the test videos were found.
            
            
              
                Conclusion:
              
              These results confirm the potential of the proposed approach in enhancing precision and reliability in microsurgical instrument tracking. Thus, the integration of attention mechanisms and a tailored data association module could be a solid base for automatizing the motion of optical microscopes.},
	language = {en},
	urldate = {2024-08-30},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Magro, Mattia and Covallero, Nicola and Gambaro, Elena and Ruffaldi, Emanuele and De Momi, Elena},
	month = aug,
	year = {2024},
}

@article{yang_image-based_2020,
	title = {Image-based laparoscopic tool detection and tracking using convolutional neural networks: a review of the literature},
	volume = {25},
	issn = {2469-9322},
	shorttitle = {Image-based laparoscopic tool detection and tracking using convolutional neural networks},
	url = {https://www.tandfonline.com/doi/full/10.1080/24699322.2020.1801842},
	doi = {10.1080/24699322.2020.1801842},
	language = {en},
	number = {1},
	urldate = {2024-08-30},
	journal = {Computer Assisted Surgery},
	author = {Yang, Congmin and Zhao, Zijian and Hu, Sanyuan},
	month = jan,
	year = {2020},
	pages = {15--28},
	file = {Full Text:/Users/scsoc/Zotero/storage/WSS85SKP/Yang et al. - 2020 - Image-based laparoscopic tool detection and tracki.pdf:application/pdf},
}

@article{castillo-segura_objective_2021,
	title = {Objective and automated assessment of surgical technical skills with {IoT} systems: {A} systematic literature review},
	volume = {112},
	issn = {09333657},
	shorttitle = {Objective and automated assessment of surgical technical skills with {IoT} systems},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0933365720312720},
	doi = {10.1016/j.artmed.2020.102007},
	language = {en},
	urldate = {2024-08-30},
	journal = {Artificial Intelligence in Medicine},
	author = {Castillo-Segura, Pablo and Fernández-Panadero, Carmen and Alario-Hoyos, Carlos and Muñoz-Merino, Pedro J. and Delgado Kloos, Carlos},
	month = feb,
	year = {2021},
	pages = {102007},
}

@article{rodrigues_surgical_2022,
	title = {Surgical {Tool} {Datasets} for {Machine} {Learning} {Research}: {A} {Survey}},
	volume = {130},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Surgical {Tool} {Datasets} for {Machine} {Learning} {Research}},
	url = {https://link.springer.com/10.1007/s11263-022-01640-6},
	doi = {10.1007/s11263-022-01640-6},
	abstract = {Abstract
            This paper is a comprehensive survey of datasets for surgical tool detection and related surgical data science and machine learning techniques and algorithms. The survey offers a high level perspective of current research in this area, analyses the taxonomy of approaches adopted by researchers using surgical tool datasets, and addresses key areas of research, such as the datasets used, evaluation metrics applied and deep learning techniques utilised. Our presentation and taxonomy provides a framework that facilitates greater understanding of current work, and highlights the challenges and opportunities for further innovative and useful research.},
	language = {en},
	number = {9},
	urldate = {2024-08-30},
	journal = {International Journal of Computer Vision},
	author = {Rodrigues, Mark and Mayo, Michael and Patros, Panos},
	month = sep,
	year = {2022},
	pages = {2222--2248},
	file = {Full Text:/Users/scsoc/Zotero/storage/4M3RDDGG/Rodrigues et al. - 2022 - Surgical Tool Datasets for Machine Learning Resear.pdf:application/pdf},
}

@inproceedings{fathabadi_surgical_2021,
	address = {Las Vegas, NV, USA},
	title = {Surgical {Skill} {Training} and {Evaluation} for a {Peg} {Transfer} {Task} of a {Three} {Camera}-{Based} {Laparoscopic} {Box}-{Trainer} {System}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66545-841-2},
	url = {https://ieeexplore.ieee.org/document/9799127/},
	doi = {10.1109/CSCI54926.2021.00242},
	urldate = {2024-08-30},
	booktitle = {2021 {International} {Conference} on {Computational} {Science} and {Computational} {Intelligence} ({CSCI})},
	publisher = {IEEE},
	author = {Fathabadi, Fatemeh Rashidi and Grantner, Janos L. and Shebrain, Saad A and Abdel-Qader, Ikhlas},
	month = dec,
	year = {2021},
	pages = {1146--1151},
}

@inproceedings{fathabadi_surgical_2021-1,
	address = {Melbourne, Australia},
	title = {Surgical {Skill} {Assessment} {System} {Using} {Fuzzy} {Logic} in a {Multi}-{Class} {Detection} of {Laparoscopic} {Box}-{Trainer} {Instruments}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-66544-207-7},
	url = {https://ieeexplore.ieee.org/document/9658766/},
	doi = {10.1109/SMC52423.2021.9658766},
	urldate = {2024-08-30},
	booktitle = {2021 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	publisher = {IEEE},
	author = {Fathabadi, Fatemeh Rashidi and Grantner, Janos L. and Shebrain, Saad A and Abdel-Qader, Ikhlas},
	month = oct,
	year = {2021},
	pages = {1248--1253},
}

@article{fathabadi_fuzzy_2022,
	title = {Fuzzy logic supervisor – {A} surgical skills assessment system using multi-class detection of laparoscopic box-trainer instruments},
	volume = {43},
	issn = {10641246, 18758967},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/JIFS-213243},
	doi = {10.3233/JIFS-213243},
	abstract = {Recent developments in deep learning can be used in skill assessments for laparoscopic surgeons. In Minimally Invasive Surgery (MIS), surgeons should acquire many skills before carrying out a real operation. The Laparoscopic Surgical Box-Trainer allows surgery residents to train on specific skills that are not traditionally taught to them. This study aims to automatically detect the tips of laparoscopic instruments, localize a point, evaluate the detection accuracy to provide valuable assessment and expedite the development of surgery skills and assess the trainees’ performance using a Multi-Input-Single-Output Fuzzy Logic Supervisor system. The output of the fuzzy logic assessment is the performance evaluation for the surgeon, and it is quantified in percentages. Based on the experimental results, the trained SSD Mobilenet V2 FPN can identify each instrument at a score of 70\% fidelity. On the other hand, the trained SSD ResNet50 V1 FPN can detect each instrument at the score of 90\% fidelity, in each location within a region of interest, and determine their relative distance with over 65\% and 80\% reliability, respectively. This method can be applied in different types of laparoscopic tooltip detection. Because there were a few instances when the detection failed, and the system was designed to generate pass-fail assessment, we recommend improving the measurement algorithm and the performance assessment by adding a camera to the system and measuring the distance from multiple perspectives.},
	number = {4},
	urldate = {2024-08-30},
	journal = {Journal of Intelligent \& Fuzzy Systems},
	author = {Fathabadi, Fatemeh Rashidi and Grantner, Janos L. and Shebrain, Saad A. and Abdel-Qader, Ikhlas},
	month = aug,
	year = {2022},
	pages = {4741--4756},
}


@article{rabbani_can_2024,
	title = {Can surgical computer vision benefit from large-scale visual foundation models?},
	volume = {19},
	issn = {1861-6429},
	url = {https://link.springer.com/10.1007/s11548-024-03125-y},
	doi = {10.1007/s11548-024-03125-y},
	language = {en},
	number = {6},
	urldate = {2024-08-30},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Rabbani, Navid and Bartoli, Adrien},
	month = apr,
	year = {2024},
	pages = {1157--1163},
}


@article{huaulme_peg_2022,
	title = {{PEg} {TRAnsfer} {Workflow} recognition challenge report: {Does} multi-modal data improve recognition?},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	shorttitle = {{PEg} {TRAnsfer} {Workflow} recognition challenge report},
	url = {https://arxiv.org/abs/2202.05821},
	doi = {10.48550/ARXIV.2202.05821},
	abstract = {This paper presents the design and results of the "PEg TRAnsfert Workflow recognition" (PETRAW) challenge whose objective was to develop surgical workflow recognition methods based on one or several modalities, among video, kinematic, and segmentation data, in order to study their added value. The PETRAW challenge provided a data set of 150 peg transfer sequences performed on a virtual simulator. This data set was composed of videos, kinematics, semantic segmentation, and workflow annotations which described the sequences at three different granularity levels: phase, step, and activity. Five tasks were proposed to the participants: three of them were related to the recognition of all granularities with one of the available modalities, while the others addressed the recognition with a combination of modalities. Average application-dependent balanced accuracy (AD-Accuracy) was used as evaluation metric to take unbalanced classes into account and because it is more clinically relevant than a frame-by-frame score. Seven teams participated in at least one task and four of them in all tasks. Best results are obtained with the use of the video and the kinematics data with an AD-Accuracy between 93\% and 90\% for the four teams who participated in all tasks. The improvement between video/kinematic-based methods and the uni-modality ones was significant for all of the teams. However, the difference in testing execution time between the video/kinematic-based and the kinematic-based methods has to be taken into consideration. Is it relevant to spend 20 to 200 times more computing time for less than 3\% of improvement? The PETRAW data set is publicly available at www.synapse.org/PETRAW to encourage further research in surgical workflow recognition.},
	urldate = {2024-08-30},
	author = {Huaulmé, Arnaud and Harada, Kanako and Nguyen, Quang-Minh and Park, Bogyu and Hong, Seungbum and Choi, Min-Kook and Peven, Michael and Li, Yunshuang and Long, Yonghao and Dou, Qi and Kumar, Satyadwyoom and Lalithkumar, Seenivasan and Hongliang, Ren and Matsuzaki, Hiroki and Ishikawa, Yuto and Harai, Yuriko and Kondo, Satoshi and Mitsuishi, Mamoru and Jannin, Pierre},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Artificial Intelligence (cs.AI), Databases (cs.DB), FOS: Computer and information sciences, Machine Learning (cs.LG)},
	annote = {Other
Challenge report doi.org/10.1016/j.cmpb.2023.107561},
}

@article{cartucho_visionblender_2021,
	title = {{VisionBlender}: a tool to efficiently generate computer vision datasets for robotic surgery},
	volume = {9},
	issn = {2168-1163, 2168-1171},
	shorttitle = {{VisionBlender}},
	url = {https://www.tandfonline.com/doi/full/10.1080/21681163.2020.1835546},
	doi = {10.1080/21681163.2020.1835546},
	language = {en},
	number = {4},
	urldate = {2024-08-30},
	journal = {Computer Methods in Biomechanics and Biomedical Engineering: Imaging \& Visualization},
	author = {Cartucho, João and Tukra, Samyakh and Li, Yunpeng and S. Elson, Daniel and Giannarou, Stamatia},
	month = jul,
	year = {2021},
	pages = {331--338},
	file = {Full Text:/Users/scsoc/Zotero/storage/PJSV9H3I/Cartucho et al. - 2021 - VisionBlender a tool to efficiently generate comp.pdf:application/pdf},
}

@inproceedings{fernandes_future_2023,
	address = {Athens, Greece},
	title = {Future {Perspectives} of {Deep} {Learning} in {Laparoscopic} {Tool} {Detection}, {Classification}, and {Segmentation}: {A} {Systematic} {Review}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350346077},
	shorttitle = {Future {Perspectives} of {Deep} {Learning} in {Laparoscopic} {Tool} {Detection}, {Classification}, and {Segmentation}},
	url = {https://ieeexplore.ieee.org/document/10253772/},
	doi = {10.1109/SeGAH57547.2023.10253772},
	urldate = {2024-08-30},
	booktitle = {2023 {IEEE} 11th {International} {Conference} on {Serious} {Games} and {Applications} for {Health} ({SeGAH})},
	publisher = {IEEE},
	author = {Fernandes, Nuno and Oliveira, Eva and Rodrigues, Nuno Feixa},
	month = aug,
	year = {2023},
	pages = {1--8},
}

@InProceedings{Zhong_2020_WACV,
author = {Zhong, Yuanyi and Wang, Jianfeng and Peng, Jian and Zhang, Lei},
title = {Anchor Box Optimization for Object Detection},
booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
month = {March},
year = {2020}
}

@article{wang_region_2019,
	title = {Region {Proposal} by {Guided} {Anchoring}},
	url = {http://arxiv.org/abs/1901.03278},
	abstract = {Region anchors are the cornerstone of modern object detection techniques. State-of-the-art detectors mostly rely on a dense anchoring scheme, where anchors are sampled uniformly over the spatial domain with a predefined set of scales and aspect ratios. In this paper, we revisit this foundational stage. Our study shows that it can be done much more effectively and efficiently. Specifically, we present an alternative scheme, named Guided Anchoring, which leverages semantic features to guide the anchoring. The proposed method jointly predicts the locations where the center of objects of interest are likely to exist as well as the scales and aspect ratios at different locations. On top of predicted anchor shapes, we mitigate the feature inconsistency with a feature adaption module. We also study the use of high-quality proposals to improve detection performance. The anchoring scheme can be seamlessly integrated into proposal methods and detectors. With Guided Anchoring, we achieve 9.1\% higher recall on MS COCO with 90\% fewer anchors than the RPN baseline. We also adopt Guided Anchoring in Fast R-CNN, Faster R-CNN and RetinaNet, respectively improving the detection mAP by 2.2\%, 2.7\% and 1.2\%. Code will be available at https://github.com/open-mmlab/mmdetection.},
	urldate = {2024-08-30},
	publisher = {arXiv},
	author = {Wang, Jiaqi and Chen, Kai and Yang, Shuo and Loy, Chen Change and Lin, Dahua},
	month = apr,
	year = {2019},
	note = {arXiv:1901.03278 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2019 camera ready},
	file = {arXiv Fulltext PDF:/Users/scsoc/Zotero/storage/AJXP22TD/Wang et al. - 2019 - Region Proposal by Guided Anchoring.pdf:application/pdf;arXiv.org Snapshot:/Users/scsoc/Zotero/storage/XPDTZB3T/1901.html:text/html},
}

@article{redmon_you_2016,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	url = {http://arxiv.org/abs/1506.02640},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
	urldate = {2024-08-30},
	publisher = {arXiv},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	month = may,
	year = {2016},
	note = {arXiv:1506.02640 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/scsoc/Zotero/storage/L7PEPDJ9/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:application/pdf},
}

@article{raja_performance_2024,
	title = {Performance {Analysis} of {YOLO}-{NAS} {SOTA} {Models} on {CAL} {Tool} {Detection}},
	copyright = {https://creativecommons.org/licenses/by-nc-sa/4.0/},
	url = {https://www.techrxiv.org/users/706970/articles/697187-performance-analysis-of-yolo-nas-sota-models-on-cal-tool-detection?commit=8538027fb57101e8163e2e679188a1fee475f5ee},
	doi = {10.36227/techrxiv.170474405.56692658/v1},
	urldate = {2024-08-30},
	author = {Raja, Muhammad Adil and Loughran, Róisín and Mccaffery, Fergal},
	month = jan,
	year = {2024},
	file = {Submitted Version:/Users/scsoc/Zotero/storage/Q6JDX3ET/Raja et al. - 2024 - Performance Analysis of YOLO-NAS SOTA Models on CA.pdf:application/pdf},
}

@article{wang_visual_2022,
	title = {Visual detection and tracking algorithms for minimally invasive surgical instruments: {A} comprehensive review of the state-of-the-art},
	volume = {149},
	issn = {09218890},
	shorttitle = {Visual detection and tracking algorithms for minimally invasive surgical instruments},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0921889021002232},
	doi = {10.1016/j.robot.2021.103945},
	language = {en},
	urldate = {2024-08-30},
	journal = {Robotics and Autonomous Systems},
	author = {Wang, Yan and Sun, Qiyuan and Liu, Zhenzhong and Gu, Lin},
	month = mar,
	year = {2022},
	pages = {103945},
}

@article{cho_automatic_2021,
	title = {Automatic tip detection of surgical instruments in biportal endoscopic spine surgery},
	volume = {133},
	issn = {00104825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010482521001785},
	doi = {10.1016/j.compbiomed.2021.104384},
	language = {en},
	urldate = {2024-08-30},
	journal = {Computers in Biology and Medicine},
	author = {Cho, Sue Min and Kim, Young-Gon and Jeong, Jinhoon and Kim, Inhwan and Lee, Ho-jin and Kim, Namkug},
	month = jun,
	year = {2021},
	pages = {104384},
	file = {Submitted Version:/Users/scsoc/Zotero/storage/2LDEGKEV/Cho et al. - 2021 - Automatic tip detection of surgical instruments in.pdf:application/pdf},
}

@inproceedings{choi_surgical-tools_2017,
	address = {Seogwipo},
	title = {Surgical-tools detection based on {Convolutional} {Neural} {Network} in laparoscopic robot-assisted surgery},
	isbn = {978-1-5090-2809-2},
	url = {https://ieeexplore.ieee.org/document/8037183/},
	doi = {10.1109/EMBC.2017.8037183},
	urldate = {2024-08-30},
	booktitle = {2017 39th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society} ({EMBC})},
	publisher = {IEEE},
	author = {Choi, Bareum and Jo, Kyungmin and Choi, Songe and Choi, Jaesoon},
	month = jul,
	year = {2017},
	pages = {1756--1759},
}

@article{jo_robust_2019,
	title = {Robust {Real}-{Time} {Detection} of {Laparoscopic} {Instruments} in {Robot} {Surgery} {Using} {Convolutional} {Neural} {Networks} with {Motion} {Vector} {Prediction}},
	volume = {9},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/9/14/2865},
	doi = {10.3390/app9142865},
	abstract = {More than half of post-operative complications can be prevented, and operation performances can be improved based on the feedback gathered from operations or notifications of the risks during operations in real time. However, existing surgical analysis methods are limited, because they involve time-consuming processes and subjective opinions. Therefore, the detection of surgical instruments is necessary for (a) conducting objective analyses, or (b) providing risk notifications associated with a surgical procedure in real time. We propose a new real-time detection algorithm for detection of surgical instruments using convolutional neural networks (CNNs). This algorithm is based on an object detection system YOLO9000 and ensures continuity of detection of the surgical tools in successive imaging frames based on motion vector prediction. This method exhibits a constant performance irrespective of a surgical instrument class, while the mean average precision (mAP) of all the tools is 84.7, with a speed of 38 frames per second (FPS).},
	language = {en},
	number = {14},
	urldate = {2024-08-30},
	journal = {Applied Sciences},
	author = {Jo, Kyungmin and Choi, Yuna and Choi, Jaesoon and Chung, Jong Woo},
	month = jul,
	year = {2019},
	pages = {2865},
	file = {Full Text:/Users/scsoc/Zotero/storage/ZL7F6PMH/Jo et al. - 2019 - Robust Real-Time Detection of Laparoscopic Instrum.pdf:application/pdf},
}

@article{benavides_real-time_2024,
	title = {Real-{Time} {Tool} {Localization} for {Laparoscopic} {Surgery} {Using} {Convolutional} {Neural} {Network}},
	volume = {24},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/24/13/4191},
	doi = {10.3390/s24134191},
	abstract = {Partially automated robotic systems, such as camera holders, represent a pivotal step towards enhancing efficiency and precision in surgical procedures. Therefore, this paper introduces an approach for real-time tool localization in laparoscopy surgery using convolutional neural networks. The proposed model, based on two Hourglass modules in series, can localize up to two surgical tools simultaneously. This study utilized three datasets: the ITAP dataset, alongside two publicly available datasets, namely Atlas Dione and EndoVis Challenge. Three variations of the Hourglass-based models were proposed, with the best model achieving high accuracy (92.86\%) and frame rates (27.64 FPS), suitable for integration into robotic systems. An evaluation on an independent test set yielded slightly lower accuracy, indicating limited generalizability. The model was further analyzed using the Grad-CAM technique to gain insights into its functionality. Overall, this work presents a promising solution for automating aspects of laparoscopic surgery, potentially enhancing surgical efficiency by reducing the need for manual endoscope manipulation.},
	language = {en},
	number = {13},
	urldate = {2024-08-30},
	journal = {Sensors},
	author = {Benavides, Diego and Cisnal, Ana and Fontúrbel, Carlos and De La Fuente, Eusebio and Fraile, Juan Carlos},
	month = jun,
	year = {2024},
	pages = {4191},
}

@misc{jocher_ultralytics_2023,
	title = {Ultralytics {YOLOv8}},
	copyright = {AGPL-3.0},
	url = {https://github.com/ultralytics/ultralytics},
	abstract = {NEW - YOLOv8 �� in PyTorch {\textgreater} ONNX {\textgreater} OpenVINO {\textgreater} CoreML {\textgreater} TFLite},
	urldate = {2024-08-30},
	author = {Jocher, Glenn and Chaurasia, Ayush and Qiu, Jing},
	month = jan,
	year = {2023},
	note = {original-date: 2022-09-11T16:39:45Z},
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1512.03385},
	doi = {10.48550/ARXIV.1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \&amp; COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2024-08-30},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2015},
	note = {Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	annote = {Other
Tech report},
}

@article{lin_focal_2017,
	title = {Focal {Loss} for {Dense} {Object} {Detection}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1708.02002},
	doi = {10.48550/ARXIV.1708.02002},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	urldate = {2024-08-30},
	publisher = {arXiv},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	year = {2017},
	note = {Version Number: 2},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
}



@article{tan_efficientdet_2019,
	title = {{EfficientDet}: {Scalable} and {Efficient} {Object} {Detection}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{EfficientDet}},
	url = {https://arxiv.org/abs/1911.09070},
	doi = {10.48550/ARXIV.1911.09070},
	abstract = {Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multiscale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and better backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single model and single-scale, our EfficientDet-D7 achieves state-of-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs, being 4x - 9x smaller and using 13x - 42x fewer FLOPs than previous detectors. Code is available at https://github.com/google/automl/tree/master/efficientdet.},
	urldate = {2024-08-30},
	author = {Tan, Mingxing and Pang, Ruoming and Le, Quoc V.},
	year = {2019},
	note = {Publisher: arXiv
Version Number: 7},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Image and Video Processing (eess.IV), Machine Learning (cs.LG)},
	annote = {Other
CVPR 2020},
}

@misc{zlocha_martinzlochaanchor-optimization_2024,
	title = {martinzlocha/anchor-optimization},
	copyright = {GPL-3.0},
	url = {https://github.com/martinzlocha/anchor-optimization},
	abstract = {Anchor optimization for RetinaNet.},
	urldate = {2024-08-30},
	author = {Zlocha, Martin},
	month = mar,
	year = {2024},
	note = {original-date: 2019-07-30T18:10:05Z},
}
