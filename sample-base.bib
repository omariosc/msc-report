@article{hasan_detection_2021,
	title = {Detection, segmentation, and {3D} pose estimation of surgical tools using convolutional neural networks and algebraic geometry},
	volume = {70},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841521000402},
	doi = {10.1016/j.media.2021.101994},
	abstract = {Background and objective:Surgical tool detection, segmentation, and 3D pose estimation are crucial components in Computer-Assisted Laparoscopy (CAL). The existing frameworks have two main limitations. First, they do not integrate all three components. Integration is critical; for instance, one should not attempt computing pose if detection is negative. Second, they have highly specific requirements, such as the availability of a CAD model. We propose an integrated and generic framework whose sole requirement for the 3D pose is that the tool shaft is cylindrical. Our framework makes the most of deep learning and geometric 3D vision by combining a proposed Convolutional Neural Network (CNN) with algebraic geometry. We show two applications of our framework in CAL: tool-aware rendering in Augmented Reality (AR) and tool-based 3D measurement. Methods:We name our CNN as ART-Net (Augmented Reality Tool Network). It has a Single Input Multiple Output (SIMO) architecture with one encoder and multiple decoders to achieve detection, segmentation, and geometric primitive extraction. These primitives are the tool edge-lines, mid-line, and tip. They allow the tool’s 3D pose to be estimated by a fast algebraic procedure. The framework only proceeds if a tool is detected. The accuracy of segmentation and geometric primitive extraction is boosted by a new Full resolution feature map Generator (FrG). We extensively evaluate the proposed framework with the EndoVis and new proposed datasets. We compare the segmentation results against several variants of the Fully Convolutional Network (FCN) and U-Net. Several ablation studies are provided for detection, segmentation, and geometric primitive extraction. The proposed datasets are surgery videos of different patients. Results:In detection, ART-Net achieves 100.0\% in both average precision and accuracy. In segmentation, it achieves 81.0\% in mean Intersection over Union (mIoU) on the robotic EndoVis dataset (articulated tool), where it outperforms both FCN and U-Net, by 4.5pp and 2.9pp, respectively. It achieves 88.2\% in mIoU on the remaining datasets (non-articulated tool). In geometric primitive extraction, ART-Net achieves 2.45∘ and 2.23∘ in mean Arc Length (mAL) error for the edge-lines and mid-line, respectively, and 9.3 pixels in mean Euclidean distance error for the tool-tip. Finally, in terms of 3D pose evaluated on animal data, our framework achieves 1.87 mm, 0.70 mm, and 4.80 mm mean absolute errors on the X, Y, and Z coordinates, respectively, and 5.94∘ angular error on the shaft orientation. It achieves 2.59 mm and 1.99 mm in mean and median location error of the tool head evaluated on patient data. Conclusions:The proposed framework outperforms existing ones in detection and segmentation. Compared to separate networks, integrating the tasks in a single network preserves accuracy in detection and segmentation but substantially improves accuracy in geometric primitive extraction. Overall, our framework has similar or better accuracy in 3D pose estimation while largely improving robustness against the very challenging imaging conditions of laparoscopy. The source code of our framework and our annotated dataset will be made publicly available at https://github.com/kamruleee51/ART-Net.},
	urldate = {2024-08-10},
	journal = {Medical Image Analysis},
	author = {Hasan, Md. Kamrul and Calvet, Lilian and Rabbani, Navid and Bartoli, Adrien},
	month = may,
	year = {2021},
	keywords = {3D pose, Algebraic geometry, Augmented reality, Computer-assisted laparoscopy, Deep learning, Segmentation},
	pages = {101994},
	file = {Full Text:/Users/scsoc/Zotero/storage/2GCQ5DNM/Hasan et al. - 2021 - Detection, segmentation, and 3D pose estimation of.pdf:application/pdf;ScienceDirect Snapshot:/Users/scsoc/Zotero/storage/S5QSR5KS/S1361841521000402.html:text/html},
}

@article{collins_tripodai_2024,
	title = {{TRIPOD}+{AI} statement: updated guidance for reporting clinical prediction models that use regression or machine learning methods},
	issn = {1756-1833},
	shorttitle = {{TRIPOD}+{AI} statement},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj-2023-078378},
	doi = {10.1136/bmj-2023-078378},
	language = {en},
	urldate = {2024-08-12},
	journal = {BMJ},
	author = {Collins, Gary S and Moons, Karel G M and Dhiman, Paula and Riley, Richard D and Beam, Andrew L and Van Calster, Ben and Ghassemi, Marzyeh and Liu, Xiaoxuan and Reitsma, Johannes B and Van Smeden, Maarten and Boulesteix, Anne-Laure and Camaradou, Jennifer Catherine and Celi, Leo Anthony and Denaxas, Spiros and Denniston, Alastair K and Glocker, Ben and Golub, Robert M and Harvey, Hugh and Heinze, Georg and Hoffman, Michael M and Kengne, André Pascal and Lam, Emily and Lee, Naomi and Loder, Elizabeth W and Maier-Hein, Lena and Mateen, Bilal A and McCradden, Melissa D and Oakden-Rayner, Lauren and Ordish, Johan and Parnell, Richard and Rose, Sherri and Singh, Karandeep and Wynants, Laure and Logullo, Patricia},
	month = apr,
	year = {2024},
	pages = {e078378},
	file = {Full Text:/Users/scsoc/Zotero/storage/QPVMUGHI/Collins et al. - 2024 - TRIPOD+AI statement updated guidance for reportin.pdf:application/pdf},
}

@article{zlocha2019improving,
  title={Improving RetinaNet for CT Lesion Detection with Dense Masks from Weak RECIST Labels},
  author={Zlocha, Martin and Dou, Qi and Glocker, Ben},
  journal={arXiv preprint arXiv:1906.02283},
  year={2019}
}

@article{bernal_comparative_2017,
	title = {Comparative {Validation} of {Polyp} {Detection} {Methods} in {Video} {Colonoscopy}: {Results} {From} the {MICCAI} 2015 {Endoscopic} {Vision} {Challenge}},
	volume = {36},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0278-0062, 1558-254X},
	shorttitle = {Comparative {Validation} of {Polyp} {Detection} {Methods} in {Video} {Colonoscopy}},
	url = {http://ieeexplore.ieee.org/document/7840040/},
	doi = {10.1109/TMI.2017.2664042},
	number = {6},
	urldate = {2024-08-17},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Bernal, Jorge and Tajkbaksh, Nima and Sanchez, Francisco Javier and Matuszewski, Bogdan J. and Chen, Hao and Yu, Lequan and Angermann, Quentin and Romain, Olivier and Rustad, Bjorn and Balasingham, Ilangko and Pogorelov, Konstantin and Choi, Sungbin and Debard, Quentin and Maier-Hein, Lena and Speidel, Stefanie and Stoyanov, Danail and Brandao, Patrick and Cordova, Henry and Sanchez-Montes, Cristina and Gurudu, Suryakanth R. and Fernandez-Esparrach, Gloria and Dray, Xavier and Liang, Jianming and Histace, Aymeric},
	month = jun,
	year = {2017},
	pages = {1231--1249},
	file = {Accepted Version:/Users/scsoc/Zotero/storage/6D559YPQ/Bernal et al. - 2017 - Comparative Validation of Polyp Detection Methods .pdf:application/pdf},
}

@article{moher_preferred_2010,
	title = {Preferred reporting items for systematic reviews and meta-analyses: {The} {PRISMA} statement},
	volume = {8},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {17439191},
	shorttitle = {Preferred reporting items for systematic reviews and meta-analyses},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1743919110000403},
	doi = {10.1016/j.ijsu.2010.02.007},
	language = {en},
	number = {5},
	urldate = {2024-08-19},
	journal = {International Journal of Surgery},
	author = {Moher, David and Liberati, Alessandro and Tetzlaff, Jennifer and Altman, Douglas G.},
	year = {2010},
	pages = {336--341},
}


@incollection{vedaldi_end--end_2020,
	address = {Cham},
	title = {End-to-{End} {Object} {Detection} with {Transformers}},
	volume = {12346},
	isbn = {978-3-030-58451-1 978-3-030-58452-8},
	url = {https://link.springer.com/10.1007/978-3-030-58452-8_13},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, eﬀectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a ﬁxed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the ﬁnal set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a uniﬁed manner. We show that it signiﬁcantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	language = {en},
	urldate = {2024-08-19},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58452-8_13},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {213--229},
	file = {Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf:/Users/scsoc/Zotero/storage/3UECQHJY/Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf:application/pdf},
}


@misc{nwoye_cholectrack20_2023L,
	title = {{CholecTrack20}: {A} {Dataset} for {Multi}-{Class} {Multiple} {Tool} {Tracking} in {Laparoscopic} {Surgery}},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	shorttitle = {{CholecTrack20}},
	url = {https://arxiv.org/abs/2312.07352},
	doi = {10.48550/ARXIV.2312.07352},
	abstract = {Tool tracking in surgical videos is vital in computer-assisted intervention for tasks like surgeon skill assessment, safety zone estimation, and human-machine collaboration during minimally invasive procedures. The lack of large-scale datasets hampers Artificial Intelligence implementation in this domain. Current datasets exhibit overly generic tracking formalization, often lacking surgical context: a deficiency that becomes evident when tools move out of the camera's scope, resulting in rigid trajectories that hinder realistic surgical representation. This paper addresses the need for a more precise and adaptable tracking formalization tailored to the intricacies of endoscopic procedures by introducing CholecTrack20, an extensive dataset meticulously annotated for multi-class multi-tool tracking across three perspectives representing the various ways of considering the temporal duration of a tool trajectory: (1) intraoperative, (2) intracorporeal, and (3) visibility within the camera's scope. The dataset comprises 20 laparoscopic videos with over 35,000 frames and 65,000 annotated tool instances with details on spatial location, category, identity, operator, phase, and surgical visual conditions. This detailed dataset caters to the evolving assistive requirements within a procedure.},
	urldate = {2024-08-20},
	publisher = {arXiv},
	author = {Nwoye, Chinedu Innocent and Elgohary, Kareem and Srinivas, Anvita and Zaid, Fauzan and Lavanchy, Joël L. and Padoy, Nicolas},
	year = {2023},
	note = {Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	annote = {Other
Surgical tool tracking dataset paper, 15 pages, 9 figures, 4 tables},
}

@article{Loza2023DTx,
   author = {Gerardo Loza and Pietro Valdastri and Sharib Ali},
   doi = {10.1049/HTL2.12060},
   issn = {2053-3713},
   journal = {Healthcare Technology Letters},
   keywords = {computer vision,medical image processing,object detection,surgery},
   publisher = {The Institution of Engineering and Technology},
   title = {Real-time surgical tool detection with multi-scale positional encoding and contrastive learning},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1049/htl2.12060 https://onlinelibrary.wiley.com/doi/abs/10.1049/htl2.12060 https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/htl2.12060},
   year = {2023},
}
@article{oldfield_assessment_1971,
	title = {The assessment and analysis of handedness: {The} {Edinburgh} inventory},
	volume = {9},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00283932},
	shorttitle = {The assessment and analysis of handedness},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0028393271900674},
	doi = {10.1016/0028-3932(71)90067-4},
	language = {en},
	number = {1},
	urldate = {2024-08-20},
	journal = {Neuropsychologia},
	author = {Oldfield, R.C.},
	month = mar,
	year = {1971},
	pages = {97--113},
}

@misc{lou_min-max_2022,
	title = {Min-{Max} {Similarity}: {A} {Contrastive} {Semi}-{Supervised} {Deep} {Learning} {Network} for {Surgical} {Tools} {Segmentation}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Min-{Max} {Similarity}},
	url = {https://arxiv.org/abs/2203.15177},
	doi = {10.48550/ARXIV.2203.15177},
	abstract = {A common problem with segmentation of medical images using neural networks is the difficulty to obtain a significant number of pixel-level annotated data for training. To address this issue, we proposed a semi-supervised segmentation network based on contrastive learning. In contrast to the previous state-of-the-art, we introduce Min-Max Similarity (MMS), a contrastive learning form of dual-view training by employing classifiers and projectors to build all-negative, and positive and negative feature pairs, respectively, to formulate the learning as solving a MMS problem. The all-negative pairs are used to supervise the networks learning from different views and to capture general features, and the consistency of unlabeled predictions is measured by pixel-wise contrastive loss between positive and negative pairs. To quantitatively and qualitatively evaluate our proposed method, we test it on four public endoscopy surgical tool segmentation datasets and one cochlear implant surgery dataset, which we manually annotated. Results indicate that our proposed method consistently outperforms state-of-the-art semi-supervised and fully supervised segmentation algorithms. And our semi-supervised segmentation algorithm can successfully recognize unknown surgical tools and provide good predictions. Also, our MMS approach could achieve inference speeds of about 40 frames per second (fps) and is suitable to deal with the real-time video segmentation.},
	urldate = {2024-08-20},
	publisher = {arXiv},
	author = {Lou, Ange and Tawfik, Kareem and Yao, Xing and Liu, Ziteng and Noble, Jack},
	year = {2022},
	note = {Version Number: 4},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Image and Video Processing (eess.IV)},
}
