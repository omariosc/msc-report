@article{hasan_detection_2021,
	title = {Detection, segmentation, and {3D} pose estimation of surgical tools using convolutional neural networks and algebraic geometry},
	volume = {70},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841521000402},
	doi = {10.1016/j.media.2021.101994},
	abstract = {Background and objective:Surgical tool detection, segmentation, and 3D pose estimation are crucial components in Computer-Assisted Laparoscopy (CAL). The existing frameworks have two main limitations. First, they do not integrate all three components. Integration is critical; for instance, one should not attempt computing pose if detection is negative. Second, they have highly specific requirements, such as the availability of a CAD model. We propose an integrated and generic framework whose sole requirement for the 3D pose is that the tool shaft is cylindrical. Our framework makes the most of deep learning and geometric 3D vision by combining a proposed Convolutional Neural Network (CNN) with algebraic geometry. We show two applications of our framework in CAL: tool-aware rendering in Augmented Reality (AR) and tool-based 3D measurement. Methods:We name our CNN as ART-Net (Augmented Reality Tool Network). It has a Single Input Multiple Output (SIMO) architecture with one encoder and multiple decoders to achieve detection, segmentation, and geometric primitive extraction. These primitives are the tool edge-lines, mid-line, and tip. They allow the tool’s 3D pose to be estimated by a fast algebraic procedure. The framework only proceeds if a tool is detected. The accuracy of segmentation and geometric primitive extraction is boosted by a new Full resolution feature map Generator (FrG). We extensively evaluate the proposed framework with the EndoVis and new proposed datasets. We compare the segmentation results against several variants of the Fully Convolutional Network (FCN) and U-Net. Several ablation studies are provided for detection, segmentation, and geometric primitive extraction. The proposed datasets are surgery videos of different patients. Results:In detection, ART-Net achieves 100.0\% in both average precision and accuracy. In segmentation, it achieves 81.0\% in mean Intersection over Union (mIoU) on the robotic EndoVis dataset (articulated tool), where it outperforms both FCN and U-Net, by 4.5pp and 2.9pp, respectively. It achieves 88.2\% in mIoU on the remaining datasets (non-articulated tool). In geometric primitive extraction, ART-Net achieves 2.45∘ and 2.23∘ in mean Arc Length (mAL) error for the edge-lines and mid-line, respectively, and 9.3 pixels in mean Euclidean distance error for the tool-tip. Finally, in terms of 3D pose evaluated on animal data, our framework achieves 1.87 mm, 0.70 mm, and 4.80 mm mean absolute errors on the X, Y, and Z coordinates, respectively, and 5.94∘ angular error on the shaft orientation. It achieves 2.59 mm and 1.99 mm in mean and median location error of the tool head evaluated on patient data. Conclusions:The proposed framework outperforms existing ones in detection and segmentation. Compared to separate networks, integrating the tasks in a single network preserves accuracy in detection and segmentation but substantially improves accuracy in geometric primitive extraction. Overall, our framework has similar or better accuracy in 3D pose estimation while largely improving robustness against the very challenging imaging conditions of laparoscopy. The source code of our framework and our annotated dataset will be made publicly available at https://github.com/kamruleee51/ART-Net.},
	urldate = {2024-08-10},
	journal = {Medical Image Analysis},
	author = {Hasan, Md. Kamrul and Calvet, Lilian and Rabbani, Navid and Bartoli, Adrien},
	month = may,
	year = {2021},
	keywords = {3D pose, Algebraic geometry, Augmented reality, Computer-assisted laparoscopy, Deep learning, Segmentation},
	pages = {101994},
	file = {Full Text:/Users/scsoc/Zotero/storage/2GCQ5DNM/Hasan et al. - 2021 - Detection, segmentation, and 3D pose estimation of.pdf:application/pdf;ScienceDirect Snapshot:/Users/scsoc/Zotero/storage/S5QSR5KS/S1361841521000402.html:text/html},
}

@misc{collins_tripodai_2024,
	title = {{TRIPOD}+{AI} statement: updated guidance for reporting clinical prediction models that use regression or machine learning methods},
	issn = {1756-1833},
	shorttitle = {{TRIPOD}+{AI} statement},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj-2023-078378},
	doi = {10.1136/bmj-2023-078378},
	language = {en},
	urldate = {2024-08-12},
	journal = {BMJ},
	author = {Collins, Gary S and Moons, Karel G M and Dhiman, Paula and Riley, Richard D and Beam, Andrew L and Van Calster, Ben and Ghassemi, Marzyeh and Liu, Xiaoxuan and Reitsma, Johannes B and Van Smeden, Maarten and Boulesteix, Anne-Laure and Camaradou, Jennifer Catherine and Celi, Leo Anthony and Denaxas, Spiros and Denniston, Alastair K and Glocker, Ben and Golub, Robert M and Harvey, Hugh and Heinze, Georg and Hoffman, Michael M and Kengne, André Pascal and Lam, Emily and Lee, Naomi and Loder, Elizabeth W and Maier-Hein, Lena and Mateen, Bilal A and McCradden, Melissa D and Oakden-Rayner, Lauren and Ordish, Johan and Parnell, Richard and Rose, Sherri and Singh, Karandeep and Wynants, Laure and Logullo, Patricia},
	month = apr,
	year = {2024},
	pages = {e078378},
	file = {Full Text:/Users/scsoc/Zotero/storage/QPVMUGHI/Collins et al. - 2024 - TRIPOD+AI statement updated guidance for reportin.pdf:application/pdf},
}

@article{alabi_multitask_2024,
	title = {Multitask {Learning} in {Minimally} {Invasive} {Surgical} {Vision}: {A} {Review}},
	abstract = {Minimally invasive surgery (MIS) has revolutionized many procedures and led to reduced recovery time and risk of patient injury. However, MIS poses additional complexity and burden on surgical teams. Data-driven surgical vision algorithms are thought to be key building blocks in the development of future MIS systems with improved autonomy. Recent advancements in machine learning and computer vision have led to successful applications in analyzing videos obtained from MIS with the promise of alleviating challenges in MIS videos.},
	language = {en},
	journal = {Medical Image Analysis},
	author = {Alabi, Oluwatosin and Vercauteren, Tom and Shi, Miaojing},
	year = {2024},
	file = {Alabi et al. - 2024 - Multitask Learning in Minimally Invasive Surgical .pdf:/Users/scsoc/Zotero/storage/AYQ46MM2/Alabi et al. - 2024 - Multitask Learning in Minimally Invasive Surgical .pdf:application/pdf},
}


@article{bernal_comparative_2017,
	title = {Comparative {Validation} of {Polyp} {Detection} {Methods} in {Video} {Colonoscopy}: {Results} {From} the {MICCAI} 2015 {Endoscopic} {Vision} {Challenge}},
	volume = {36},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0278-0062, 1558-254X},
	shorttitle = {Comparative {Validation} of {Polyp} {Detection} {Methods} in {Video} {Colonoscopy}},
	url = {http://ieeexplore.ieee.org/document/7840040/},
	doi = {10.1109/TMI.2017.2664042},
	number = {6},
	urldate = {2024-08-17},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Bernal, Jorge and Tajkbaksh, Nima and Sanchez, Francisco Javier and Matuszewski, Bogdan J. and Chen, Hao and Yu, Lequan and Angermann, Quentin and Romain, Olivier and Rustad, Bjorn and Balasingham, Ilangko and Pogorelov, Konstantin and Choi, Sungbin and Debard, Quentin and Maier-Hein, Lena and Speidel, Stefanie and Stoyanov, Danail and Brandao, Patrick and Cordova, Henry and Sanchez-Montes, Cristina and Gurudu, Suryakanth R. and Fernandez-Esparrach, Gloria and Dray, Xavier and Liang, Jianming and Histace, Aymeric},
	month = jun,
	year = {2017},
	pages = {1231--1249},
	file = {Accepted Version:/Users/scsoc/Zotero/storage/6D559YPQ/Bernal et al. - 2017 - Comparative Validation of Polyp Detection Methods .pdf:application/pdf},
}

@article{moher_preferred_2010,
	title = {Preferred reporting items for systematic reviews and meta-analyses: {The} {PRISMA} statement},
	volume = {8},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {17439191},
	shorttitle = {Preferred reporting items for systematic reviews and meta-analyses},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1743919110000403},
	doi = {10.1016/j.ijsu.2010.02.007},
	language = {en},
	number = {5},
	urldate = {2024-08-19},
	journal = {International Journal of Surgery},
	author = {Moher, David and Liberati, Alessandro and Tetzlaff, Jennifer and Altman, Douglas G.},
	year = {2010},
	pages = {336--341},
}

@misc{ali2023comprehensivesurveyrecentdeep,
      title={A comprehensive survey on recent deep learning-based methods applied to surgical data}, 
      author={Mansoor Ali and Rafael Martinez Garcia Pena and Gilberto Ochoa Ruiz and Sharib Ali},
      year={2023},
      eprint={2209.01435},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2209.01435}, 
}

@InProceedings{10.1007/978-3-030-58452-8_13,
author="Carion, Nicolas
and Massa, Francisco
and Synnaeve, Gabriel
and Usunier, Nicolas
and Kirillov, Alexander
and Zagoruyko, Sergey",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="End-to-End Object Detection with Transformers",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="213--229",
abstract="We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.",
isbn="978-3-030-58452-8"
}


@misc{nwoye_cholectrack20_2023,
	title = {{CholecTrack20}: {A} {Dataset} for {Multi}-{Class} {Multiple} {Tool} {Tracking} in {Laparoscopic} {Surgery}},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	shorttitle = {{CholecTrack20}},
	url = {https://arxiv.org/abs/2312.07352},
	doi = {10.48550/ARXIV.2312.07352},
	abstract = {Tool tracking in surgical videos is vital in computer-assisted intervention for tasks like surgeon skill assessment, safety zone estimation, and human-machine collaboration during minimally invasive procedures. The lack of large-scale datasets hampers Artificial Intelligence implementation in this domain. Current datasets exhibit overly generic tracking formalization, often lacking surgical context: a deficiency that becomes evident when tools move out of the camera's scope, resulting in rigid trajectories that hinder realistic surgical representation. This paper addresses the need for a more precise and adaptable tracking formalization tailored to the intricacies of endoscopic procedures by introducing CholecTrack20, an extensive dataset meticulously annotated for multi-class multi-tool tracking across three perspectives representing the various ways of considering the temporal duration of a tool trajectory: (1) intraoperative, (2) intracorporeal, and (3) visibility within the camera's scope. The dataset comprises 20 laparoscopic videos with over 35,000 frames and 65,000 annotated tool instances with details on spatial location, category, identity, operator, phase, and surgical visual conditions. This detailed dataset caters to the evolving assistive requirements within a procedure.},
	urldate = {2024-08-20},
	publisher = {arXiv},
	author = {Nwoye, Chinedu Innocent and Elgohary, Kareem and Srinivas, Anvita and Zaid, Fauzan and Lavanchy, Joël L. and Padoy, Nicolas},
	year = {2023},
	note = {Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	annote = {Other
Surgical tool tracking dataset paper, 15 pages, 9 figures, 4 tables},
}

@article{oldfield_assessment_1971,
	title = {The assessment and analysis of handedness: {The} {Edinburgh} inventory},
	volume = {9},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00283932},
	shorttitle = {The assessment and analysis of handedness},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0028393271900674},
	doi = {10.1016/0028-3932(71)90067-4},
	language = {en},
	number = {1},
	urldate = {2024-08-20},
	journal = {Neuropsychologia},
	author = {Oldfield, R.C.},
	month = mar,
	year = {1971},
	pages = {97--113},
}

@misc{lou_min-max_2022,
	title = {Min-{Max} {Similarity}: {A} {Contrastive} {Semi}-{Supervised} {Deep} {Learning} {Network} for {Surgical} {Tools} {Segmentation}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Min-{Max} {Similarity}},
	url = {https://arxiv.org/abs/2203.15177},
	doi = {10.48550/ARXIV.2203.15177},
	abstract = {A common problem with segmentation of medical images using neural networks is the difficulty to obtain a significant number of pixel-level annotated data for training. To address this issue, we proposed a semi-supervised segmentation network based on contrastive learning. In contrast to the previous state-of-the-art, we introduce Min-Max Similarity (MMS), a contrastive learning form of dual-view training by employing classifiers and projectors to build all-negative, and positive and negative feature pairs, respectively, to formulate the learning as solving a MMS problem. The all-negative pairs are used to supervise the networks learning from different views and to capture general features, and the consistency of unlabeled predictions is measured by pixel-wise contrastive loss between positive and negative pairs. To quantitatively and qualitatively evaluate our proposed method, we test it on four public endoscopy surgical tool segmentation datasets and one cochlear implant surgery dataset, which we manually annotated. Results indicate that our proposed method consistently outperforms state-of-the-art semi-supervised and fully supervised segmentation algorithms. And our semi-supervised segmentation algorithm can successfully recognize unknown surgical tools and provide good predictions. Also, our MMS approach could achieve inference speeds of about 40 frames per second (fps) and is suitable to deal with the real-time video segmentation.},
	urldate = {2024-08-20},
	publisher = {arXiv},
	author = {Lou, Ange and Tawfik, Kareem and Yao, Xing and Liu, Ziteng and Noble, Jack},
	year = {2022},
	note = {Version Number: 4},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Image and Video Processing (eess.IV)},
}

@article{fathabadi_box-trainer_2022,
	title = {Box-{Trainer} {Assessment} {System} with {Real}-{Time} {Multi}-{Class} {Detection} and {Tracking} of {Laparoscopic} {Instruments}, using {CNN}},
	volume = {19},
	abstract = {In Minimally Invasive Surgery (MIS), surgeons need to acquire a specific set of skills, before carrying out a “real” operation. Training with the Laparoscopic Surgical BoxTrainer device helps in acquiring the needed skills for surgery residents which are traditionally not taught to them. Video recording of residents’ performance and computerassisted surgical trainers for MIS provide valuable information for resident’s assessment. In this paper, we propose real-time detection and tracking of a multi-class of laparoscopic instruments for an intelligent box-trainer performance assessment system using SSDResNet50 V1 FPN architecture in TensorFlow backend. The dataset has been extracted from various laparoscopic box training videos. Using distance measurements and evaluation criteria constraints, we present an evaluation of the surgeon’s performance. Based on the experimental result, the trained model could identify each instrument at the score of 90\% fidelity, in each location, within a region of interest. This research is a result of a partnership between the Department of Electrical and Computer Engineering and the Department of Surgery, of the Homer Stryker M.D. School of Medicine, at Western Michigan University.},
	language = {en},
	number = {2},
	journal = {Acta Polytechnica Hungarica},
	author = {Fathabadi, Fatemeh Rashidi and Grantner, Janos L},
	year = {2022},
	file = {Fathabadi and Grantner - 2022 - Box-Trainer Assessment System with Real-Time Multi.pdf:/Users/scsoc/Zotero/storage/CD6YJCRH/Fathabadi and Grantner - 2022 - Box-Trainer Assessment System with Real-Time Multi.pdf:application/pdf},
}

@inproceedings{jin_tool_2018,
	address = {Lake Tahoe, NV},
	title = {Tool {Detection} and {Operative} {Skill} {Assessment} in {Surgical} {Videos} {Using} {Region}-{Based} {Convolutional} {Neural} {Networks}},
	isbn = {978-1-5386-4886-5},
	url = {https://ieeexplore.ieee.org/document/8354185/},
	doi = {10.1109/WACV.2018.00081},
	urldate = {2024-08-21},
	booktitle = {2018 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Jin, Amy and Yeung, Serena and Jopling, Jeffrey and Krause, Jonathan and Azagury, Dan and Milstein, Arnold and Fei-Fei, Li},
	month = mar,
	year = {2018},
	pages = {691--699},
	file = {Submitted Version:/Users/scsoc/Zotero/storage/24DSBGM8/Jin et al. - 2018 - Tool Detection and Operative Skill Assessment in S.pdf:application/pdf},
}


@article{constable_enhancing_2024,
	title = {Enhancing surgical performance in cardiothoracic surgery with innovations from computer vision and artificial intelligence: a narrative review},
	volume = {19},
	issn = {1749-8090},
	shorttitle = {Enhancing surgical performance in cardiothoracic surgery with innovations from computer vision and artificial intelligence},
	url = {https://doi.org/10.1186/s13019-024-02558-5},
	doi = {10.1186/s13019-024-02558-5},
	abstract = {When technical requirements are high, and patient outcomes are critical, opportunities for monitoring and improving surgical skills via objective motion analysis feedback may be particularly beneficial. This narrative review synthesises work on technical and non-technical surgical skills, collaborative task performance, and pose estimation to illustrate new opportunities to advance cardiothoracic surgical performance with innovations from computer vision and artificial intelligence. These technological innovations are critically evaluated in terms of the benefits they could offer the cardiothoracic surgical community, and any barriers to the uptake of the technology are elaborated upon. Like some other specialities, cardiothoracic surgery has relatively few opportunities to benefit from tools with data capture technology embedded within them (as is possible with robotic-assisted laparoscopic surgery, for example). In such cases, pose estimation techniques that allow for movement tracking across a conventional operating field without using specialist equipment or markers offer considerable potential. With video data from either simulated or real surgical procedures, these tools can (1) provide insight into the development of expertise and surgical performance over a surgeon’s career, (2) provide feedback to trainee surgeons regarding areas for improvement, (3) provide the opportunity to investigate what aspects of skill may be linked to patient outcomes which can (4) inform the aspects of surgical skill which should be focused on within training or mentoring programmes. Classifier or assessment algorithms that use artificial intelligence to ‘learn’ what expertise is from expert surgical evaluators could further assist educators in determining if trainees meet competency thresholds. With collaborative efforts between surgical teams, medical institutions, computer scientists and researchers to ensure this technology is developed with usability and ethics in mind, the developed feedback tools could improve cardiothoracic surgical practice in a data-driven way.},
	number = {1},
	urldate = {2024-08-21},
	journal = {Journal of Cardiothoracic Surgery},
	author = {Constable, Merryn D. and Shum, Hubert P. H. and Clark, Stephen},
	month = feb,
	year = {2024},
	keywords = {Deep learning, Markerless motion tracking, Pose estimation, Psychomotor ability, Surgical education, Surgical expertise, Surgical kinematics, Surgical performance, Surgical skills, Surgical training},
	pages = {94},
	file = {Full Text PDF:/Users/scsoc/Zotero/storage/ZBQBKGUK/Constable et al. - 2024 - Enhancing surgical performance in cardiothoracic s.pdf:application/pdf;Snapshot:/Users/scsoc/Zotero/storage/MVXLNC7N/s13019-024-02558-5.html:text/html},
}



@article{jaffray_minimally_2005,
	title = {Minimally invasive surgery},
	volume = {90},
	issn = {0003-9888, 1468-2044},
	url = {https://adc.bmj.com/lookup/doi/10.1136/adc.2004.062760},
	doi = {10.1136/adc.2004.062760},
	language = {en},
	number = {5},
	urldate = {2024-08-22},
	journal = {Archives of Disease in Childhood},
	author = {Jaffray, B},
	month = may,
	year = {2005},
	pages = {537--542},
	file = {Full Text:/Users/scsoc/Zotero/storage/ZC25V6XF/Jaffray - 2005 - Minimally invasive surgery.pdf:application/pdf},
}

@article{monnet_laparoscopy_2003,
	title = {Laparoscopy},
	volume = {33},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {01955616},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0195561603000585},
	doi = {10.1016/S0195-5616(03)00058-5},
	language = {en},
	number = {5},
	urldate = {2024-08-22},
	journal = {Veterinary Clinics of North America: Small Animal Practice},
	author = {Monnet, Eric and Twedt, David C},
	month = sep,
	year = {2003},
	pages = {1147--1163},
}

@article{rockall_laparoscopy_2014,
	title = {Laparoscopy in the era of enhanced recovery},
	volume = {28},
	issn = {15216918},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S152169181300228X},
	doi = {10.1016/j.bpg.2013.11.001},
	language = {en},
	number = {1},
	urldate = {2024-08-22},
	journal = {Best Practice \& Research Clinical Gastroenterology},
	author = {Rockall, T.A. and Demartines, N.},
	month = feb,
	year = {2014},
	pages = {133--142},
}

@article{meara_global_2015,
	title = {Global {Surgery} 2030: evidence and solutions for achieving health, welfare, and economic development},
	volume = {386},
	issn = {01406736},
	shorttitle = {Global {Surgery} 2030},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S014067361560160X},
	doi = {10.1016/S0140-6736(15)60160-X},
	language = {en},
	number = {9993},
	urldate = {2024-08-22},
	journal = {The Lancet},
	author = {Meara, John G and Leather, Andrew J M and Hagander, Lars and Alkire, Blake C and Alonso, Nivaldo and Ameh, Emmanuel A and Bickler, Stephen W and Conteh, Lesong and Dare, Anna J and Davies, Justine and Mérisier, Eunice Dérivois and El-Halabi, Shenaaz and Farmer, Paul E and Gawande, Atul and Gillies, Rowan and Greenberg, Sarah L M and Grimes, Caris E and Gruen, Russell L and Ismail, Edna Adan and Kamara, Thaim Buya and Lavy, Chris and Lundeg, Ganbold and Mkandawire, Nyengo C and Raykar, Nakul P and Riesel, Johanna N and Rodas, Edgar and Rose, John and Roy, Nobhojit and Shrime, Mark G and Sullivan, Richard and Verguet, Stéphane and Watters, David and Weiser, Thomas G and Wilson, Iain H and Yamey, Gavin and Yip, Winnie},
	month = aug,
	year = {2015},
	pages = {569--624},
	file = {Full Text:/Users/scsoc/Zotero/storage/YBHW8KUK/Meara et al. - 2015 - Global Surgery 2030 evidence and solutions for ac.pdf:application/pdf},
}


@book{organization_health_2016,
	title = {Health workforce requirements for universal health coverage and the {Sustainable} {Development} {Goals}. ({Human} {Resources} for {Health} {Observer}, 17)},
	isbn = {978-92-4-151140-7},
	url = {https://iris.who.int/handle/10665/250330},
	abstract = {40 p.},
	language = {en},
	urldate = {2024-08-22},
	publisher = {World Health Organization},
	author = {Organization, World Health},
	year = {2016},
	note = {Accepted: 2016-10-10T10:31:06Z},
	file = {Full Text PDF:/Users/scsoc/Zotero/storage/VPRJLRKK/Organization - 2016 - Health workforce requirements for universal health.pdf:application/pdf},
}

@article{maier-hein_surgical_2022,
	title = {Surgical data science – from concepts toward clinical translation},
	volume = {76},
	issn = {13618415},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841521003510},
	doi = {10.1016/j.media.2021.102306},
	language = {en},
	urldate = {2024-08-22},
	journal = {Medical Image Analysis},
	author = {Maier-Hein, Lena and Eisenmann, Matthias and Sarikaya, Duygu and März, Keno and Collins, Toby and Malpani, Anand and Fallert, Johannes and Feussner, Hubertus and Giannarou, Stamatia and Mascagni, Pietro and Nakawala, Hirenkumar and Park, Adrian and Pugh, Carla and Stoyanov, Danail and Vedula, Swaroop S. and Cleary, Kevin and Fichtinger, Gabor and Forestier, Germain and Gibaud, Bernard and Grantcharov, Teodor and Hashizume, Makoto and Heckmann-Nötzel, Doreen and Kenngott, Hannes G. and Kikinis, Ron and Mündermann, Lars and Navab, Nassir and Onogur, Sinan and Roß, Tobias and Sznitman, Raphael and Taylor, Russell H. and Tizabi, Minu D. and Wagner, Martin and Hager, Gregory D. and Neumuth, Thomas and Padoy, Nicolas and Collins, Justin and Gockel, Ines and Goedeke, Jan and Hashimoto, Daniel A. and Joyeux, Luc and Lam, Kyle and Leff, Daniel R. and Madani, Amin and Marcus, Hani J. and Meireles, Ozanan and Seitel, Alexander and Teber, Dogu and Ückert, Frank and Müller-Stich, Beat P. and Jannin, Pierre and Speidel, Stefanie},
	month = feb,
	year = {2022},
	pages = {102306},
	file = {Full Text:/Users/scsoc/Zotero/storage/FQ6ASMMY/Maier-Hein et al. - 2022 - Surgical data science – from concepts toward clini.pdf:application/pdf},
}

@book{world_health_organization_world_2016,
	address = {Geneva},
	title = {World health statistics 2016: monitoring health for the {SDGs}, sustainable development goals},
	isbn = {978-92-4-156526-4},
	shorttitle = {World health statistics 2016},
	url = {https://iris.who.int/handle/10665/206498},
	language = {en},
	urldate = {2024-08-22},
	publisher = {World Health Organization},
	author = {{World Health Organization}},
	year = {2016},
	keywords = {Global Health, Health Priorities, Health Status Indicators, Life Expectancy, Mortality, Organizational Objectives, Statistics, Universal Health Insurance},
	file = {World Health Organization - 2016 - World health statistics 2016 monitoring health fo.pdf:/Users/scsoc/Zotero/storage/DX58HYK4/World Health Organization - 2016 - World health statistics 2016 monitoring health fo.pdf:application/pdf},
}

@article{vassiliou_global_2005,
	title = {A global assessment tool for evaluation of intraoperative laparoscopic skills},
	volume = {190},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00029610},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0002961005003478},
	doi = {10.1016/j.amjsurg.2005.04.004},
	language = {en},
	number = {1},
	urldate = {2024-08-22},
	journal = {The American Journal of Surgery},
	author = {Vassiliou, Melina C. and Feldman, Liane S. and Andrew, Christopher G. and Bergman, Simon and Leffondré, Karen and Stanbridge, Donna and Fried, Gerald M.},
	month = jul,
	year = {2005},
	pages = {107--113},
}

@article{jones_analysis_2018,
	title = {Analysis of {Mechanical} {Forces} {Used} {During} {Laparoscopic} {Training} {Procedures}},
	volume = {32},
	copyright = {http://www.liebertpub.com/nv/resources-tools/text-and-data-mining-policy/121/},
	issn = {0892-7790, 1557-900X},
	url = {http://www.liebertpub.com/doi/10.1089/end.2017.0894},
	doi = {10.1089/end.2017.0894},
	language = {en},
	number = {6},
	urldate = {2024-08-22},
	journal = {Journal of Endourology},
	author = {Jones, Dominic and Jaffer, Ata and Nodeh, Ali Alazmani and Biyani, Chandra Shekhar and Culmer, Peter},
	month = jun,
	year = {2018},
	pages = {529--533},
	file = {Accepted Version:/Users/scsoc/Zotero/storage/2ILEXCUP/Jones et al. - 2018 - Analysis of Mechanical Forces Used During Laparosc.pdf:application/pdf},
}

@article{retrosi_motion_2015,
	title = {Motion {Analysis}–{Based} {Skills} {Training} and {Assessment} in {Pediatric} {Laparoscopy}: {Construct}, {Concurrent}, and {Content} {Validity} for the {eoSim} {Simulator}},
	volume = {25},
	copyright = {http://www.liebertpub.com/nv/resources-tools/text-and-data-mining-policy/121/},
	issn = {1092-6429, 1557-9034},
	shorttitle = {Motion {Analysis}–{Based} {Skills} {Training} and {Assessment} in {Pediatric} {Laparoscopy}},
	url = {http://www.liebertpub.com/doi/10.1089/lap.2015.0069},
	doi = {10.1089/lap.2015.0069},
	language = {en},
	number = {11},
	urldate = {2024-08-22},
	journal = {Journal of Laparoendoscopic \& Advanced Surgical Techniques},
	author = {Retrosi, Giuseppe and Cundy, Thomas and Haddad, Munther and Clarke, Simon},
	month = nov,
	year = {2015},
	pages = {944--950},
}

@article{levin_automated_2019,
	title = {Automated {Methods} of {Technical} {Skill} {Assessment} in {Surgery}: {A} {Systematic} {Review}},
	volume = {76},
	issn = {19317204},
	shorttitle = {Automated {Methods} of {Technical} {Skill} {Assessment} in {Surgery}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1931720419301643},
	doi = {10.1016/j.jsurg.2019.06.011},
	language = {en},
	number = {6},
	urldate = {2024-08-22},
	journal = {Journal of Surgical Education},
	author = {Levin, Marc and McKechnie, Tyler and Khalid, Shuja and Grantcharov, Teodor P. and Goldenberg, Mitchell},
	month = nov,
	year = {2019},
	pages = {1629--1639},
}

@article{paley_crowdsourced_2021,
	title = {Crowdsourced {Assessment} of {Surgical} {Skill} {Proficiency} in {Cataract} {Surgery}},
	volume = {78},
	issn = {19317204},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1931720421000477},
	doi = {10.1016/j.jsurg.2021.02.004},
	language = {en},
	number = {4},
	urldate = {2024-08-22},
	journal = {Journal of Surgical Education},
	author = {Paley, Grace L. and Grove, Rebecca and Sekhar, Tejas C. and Pruett, Jack and Stock, Michael V. and Pira, Tony N. and Shields, Steven M. and Waxman, Evan L. and Wilson, Bradley S. and Gordon, Mae O. and Culican, Susan M.},
	month = jul,
	year = {2021},
	pages = {1077--1088},
	file = {Full Text:/Users/scsoc/Zotero/storage/3434UHBP/Paley et al. - 2021 - Crowdsourced Assessment of Surgical Skill Proficie.pdf:application/pdf},
}

@article{allan_toward_2013,
	title = {Toward {Detection} and {Localization} of {Instruments} in {Minimally} {Invasive} {Surgery}},
	volume = {60},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0018-9294, 1558-2531},
	url = {http://ieeexplore.ieee.org/document/6359786/},
	doi = {10.1109/TBME.2012.2229278},
	number = {4},
	urldate = {2024-08-22},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {Allan, M. and Ourselin, S. and Thompson, S. and Hawkes, D. J. and Kelly, J. and Stoyanov, D.},
	month = apr,
	year = {2013},
	pages = {1050--1058},
}

@misc{bodenstedt_comparative_2018,
	title = {Comparative evaluation of instrument segmentation and tracking methods in minimally invasive surgery},
	url = {http://arxiv.org/abs/1805.02475},
	abstract = {Intraoperative segmentation and tracking of minimally invasive instruments is a prerequisite for computer- and robotic-assisted surgery. Since additional hardware like tracking systems or the robot encoders are cumbersome and lack accuracy, surgical vision is evolving as promising techniques to segment and track the instruments using only the endoscopic images. However, what is missing so far are common image data sets for consistent evaluation and benchmarking of algorithms against each other. The paper presents a comparative validation study of different vision-based methods for instrument segmentation and tracking in the context of robotic as well as conventional laparoscopic surgery. The contribution of the paper is twofold: we introduce a comprehensive validation data set that was provided to the study participants and present the results of the comparative validation study. Based on the results of the validation study, we arrive at the conclusion that modern deep learning approaches outperform other methods in instrument segmentation tasks, but the results are still not perfect. Furthermore, we show that merging results from different methods actually significantly increases accuracy in comparison to the best stand-alone method. On the other hand, the results of the instrument tracking task show that this is still an open challenge, especially during challenging scenarios in conventional laparoscopic surgery.},
	urldate = {2024-08-22},
	publisher = {arXiv},
	author = {Bodenstedt, Sebastian and Allan, Max and Agustinos, Anthony and Du, Xiaofei and Garcia-Peraza-Herrera, Luis and Kenngott, Hannes and Kurmann, Thomas and Müller-Stich, Beat and Ourselin, Sebastien and Pakhomov, Daniil and Sznitman, Raphael and Teichmann, Marvin and Thoma, Martin and Vercauteren, Tom and Voros, Sandrine and Wagner, Martin and Wochner, Pamela and Maier-Hein, Lena and Stoyanov, Danail and Speidel, Stefanie},
	month = may,
	year = {2018},
	note = {arXiv:1805.02475 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/scsoc/Zotero/storage/TFPU57DQ/Bodenstedt et al. - 2018 - Comparative evaluation of instrument segmentation .pdf:application/pdf;arXiv.org Snapshot:/Users/scsoc/Zotero/storage/Z6F6HZY7/1805.html:text/html},
}

@article{loza_realtime_2024,
	title = {Real‐time surgical tool detection with multi‐scale positional encoding and contrastive learning},
	volume = {11},
	issn = {2053-3713, 2053-3713},
	url = {https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/htl2.12060},
	doi = {10.1049/htl2.12060},
	abstract = {Abstract
            Real‐time detection of surgical tools in laparoscopic data plays a vital role in understanding surgical procedures, evaluating the performance of trainees, facilitating learning, and ultimately supporting the autonomy of robotic systems. Existing detection methods for surgical data need to improve processing speed and high prediction accuracy. Most methods rely on anchors or region proposals, limiting their adaptability to variations in tool appearance and leading to sub‐optimal detection results. Moreover, using non‐anchor‐based detectors to alleviate this problem has been partially explored without remarkable results. An anchor‐free architecture based on a transformer that allows real‐time tool detection is introduced. The proposal is to utilize multi‐scale features within the feature extraction layer and at the transformer‐based detection architecture through positional encoding that can refine and capture context‐aware and structural information of different‐sized tools. Furthermore, a supervised contrastive loss is introduced to optimize representations of object embeddings, resulting in improved feed‐forward network performances for classifying localized bounding boxes. The strategy demonstrates superiority to state‐of‐the‐art (SOTA) methods. Compared to the most accurate existing SOTA (DSSS) method, the approach has an improvement of nearly 4\% on mAP and a reduction in the inference time by 113\%. It also showed a 7\% higher mAP than the baseline model.},
	language = {en},
	number = {2-3},
	urldate = {2024-08-22},
	journal = {Healthcare Technology Letters},
	author = {Loza, Gerardo and Valdastri, Pietro and Ali, Sharib},
	month = apr,
	year = {2024},
	pages = {48--58},
}

@article{bouget_vision-based_2017,
	title = {Vision-based and marker-less surgical tool detection and tracking: a review of the literature},
	volume = {35},
	issn = {13618415},
	shorttitle = {Vision-based and marker-less surgical tool detection and tracking},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841516301657},
	doi = {10.1016/j.media.2016.09.003},
	language = {en},
	urldate = {2024-08-22},
	journal = {Medical Image Analysis},
	author = {Bouget, David and Allan, Max and Stoyanov, Danail and Jannin, Pierre},
	month = jan,
	year = {2017},
	pages = {633--654},
	file = {Submitted Version:/Users/scsoc/Zotero/storage/KQWNXPFJ/Bouget et al. - 2017 - Vision-based and marker-less surgical tool detecti.pdf:application/pdf},
}
@incollection{koonce_resnet_2021,
	address = {Berkeley, CA},
	title = {{ResNet} 50},
	isbn = {978-1-4842-6167-5 978-1-4842-6168-2},
	url = {http://link.springer.com/10.1007/978-1-4842-6168-2_6},
	language = {en},
	urldate = {2024-08-23},
	booktitle = {Convolutional {Neural} {Networks} with {Swift} for {Tensorflow}},
	publisher = {Apress},
	author = {Koonce, Brett},
	collaborator = {Koonce, Brett},
	year = {2021},
	doi = {10.1007/978-1-4842-6168-2_6},
	pages = {63--72},
}

@article{teevno_semi-supervised_2023,
	title = {A semi-supervised {Teacher}-{Student} framework for surgical tool detection and localization},
	volume = {11},
	issn = {2168-1163, 2168-1171},
	url = {https://www.tandfonline.com/doi/full/10.1080/21681163.2022.2150688},
	doi = {10.1080/21681163.2022.2150688},
	language = {en},
	number = {4},
	urldate = {2024-08-24},
	journal = {Computer Methods in Biomechanics and Biomedical Engineering: Imaging \& Visualization},
	author = {Teevno, Mansoor Ali and Ochoa-Ruiz, Gilberto and Ali, Sharib},
	month = jul,
	year = {2023},
	pages = {1033--1041},
}

@article{martin_objective_1997,
	title = {Objective structured assessment of technical skill ({OSATS}) for surgical residents: {OBJECTIVE} {STRUCTURED} {ASSESSMENT} {OF} {TECHNICAL} {SKILL}},
	volume = {84},
	copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
	issn = {00071323},
	shorttitle = {Objective structured assessment of technical skill ({OSATS}) for surgical residents},
	url = {https://academic.oup.com/bjs/article/84/2/273/6167081},
	doi = {10.1046/j.1365-2168.1997.02502.x},
	language = {en},
	number = {2},
	urldate = {2024-08-24},
	journal = {British Journal of Surgery},
	author = {Martin, J. A. and Regehr, G. and Reznick, R. and Macrae, H. and Murnaghan, J. and Hutchison, C. and Brown, M.},
	month = feb,
	year = {1997},
	pages = {273--278},
}

@article{hussein_development_2017,
	title = {Development and {Validation} of an {Objective} {Scoring} {Tool} for {Robot}-{Assisted} {Radical} {Prostatectomy}: {Prostatectomy} {Assessment} and {Competency} {Evaluation}},
	volume = {197},
	issn = {0022-5347, 1527-3792},
	shorttitle = {Development and {Validation} of an {Objective} {Scoring} {Tool} for {Robot}-{Assisted} {Radical} {Prostatectomy}},
	url = {http://www.jurology.com/doi/10.1016/j.juro.2016.11.100},
	doi = {10.1016/j.juro.2016.11.100},
	language = {en},
	number = {5},
	urldate = {2024-08-24},
	journal = {Journal of Urology},
	author = {Hussein, Ahmed A. and Ghani, Khurshid R. and Peabody, James and Sarle, Richard and Abaza, Ronney and Eun, Daniel and Hu, Jim and Fumo, Michael and Lane, Brian and Montgomery, Jeffrey S. and Hinata, Nobuyuki and Rooney, Deborah and Comstock, Bryan and Chan, Hei Kit and Mane, Sridhar S. and Mohler, James L. and Wilding, Gregory and Miller, David and Guru, Khurshid A. and {Michigan Urological Surgery Improvement Collaborative and Applied Technology Laboratory for Advanced Surgery Program}},
	month = may,
	year = {2017},
	pages = {1237--1244},
}

@article{hilal_randomized_2017,
	title = {A randomized comparison of video demonstration versus hands-on training of medical students for vacuum delivery using {Objective} {Structured} {Assessment} of {Technical} {Skills} ({OSATS})},
	volume = {96},
	issn = {0025-7974},
	url = {https://journals.lww.com/00005792-201703170-00052},
	doi = {10.1097/MD.0000000000006355},
	language = {en},
	number = {11},
	urldate = {2024-08-24},
	journal = {Medicine},
	author = {Hilal, Ziad and Kumpernatz, Anne K. and Rezniczek, Günther A. and Cetin, Cem and Tempfer-Bentz, Eva-Katrin and Tempfer, Clemens B.},
	month = mar,
	year = {2017},
	pages = {e6355},
	file = {Full Text:/Users/scsoc/Zotero/storage/RKX45GPC/Hilal et al. - 2017 - A randomized comparison of video demonstration ver.pdf:application/pdf},
}

@article{pears_capturing_2021,
	title = {Capturing the non-technical skills of a technical skills trainer ({NTS}-{TeST}) during simulation},
	volume = {66},
	issn = {0036-9330, 2045-6441},
	url = {http://journals.sagepub.com/doi/10.1177/00369330211008594},
	doi = {10.1177/00369330211008594},
	abstract = {Objective
              To develop an assessment instrument that can be used as a comprehensive feedback record to convey to a trainer the non-technical aspects of skill acquisition and training.
            
            
              Methods
              The instrument was developed across three rounds. In Round 1, 6 endourological consultants undertook a modified Delphi process. Round 2 included 10 trainers who assessed each question’s relevance and practicability. Round 3 involved a pilot study with fifteen urology residents who participated in a technical skills simulation session with the incorporation of the instrument. We report the content, face, and construct validity, and the internal consistency of an NTS instrument for trainers.
            
            
              Results
              The instrument had a consistent and a high positive average for each of the 4 sections of the instrument, regardless of the type of user. Positive Spearman’s correlation coefficients (0.02 to .64) for content validity and Cronbach’s alpha (a = 0.70) indicated good validity and moderate reliability of the instrument.
            
            
              Conclusion
              We propose a novel NTS instrument for trainers during a simulation. This instrument can be used for benchmarking the quality of technical skills simulation training.},
	language = {en},
	number = {3},
	urldate = {2024-08-24},
	journal = {Scottish Medical Journal},
	author = {Pears, Matthew and Biyani, Chandra Shekhar and Joyce, Adrian D and Spearpoint, Ken and Yiasemidou, Marina and Cleynenbreugel, Ben Van and Patterson, Jake and Mushtaq, Faisal},
	month = aug,
	year = {2021},
	pages = {124--133},
}

@misc{noauthor_yolov10_nodate,
	title = {{YOLOv10} - {Ultralytics} {YOLO} {Docs}},
	url = {https://docs.ultralytics.com/models/yolov10/},
	urldate = {2024-08-25},
	file = {YOLOv10 - Ultralytics YOLO Docs:/Users/scsoc/Zotero/storage/75BLN7M7/yolov10.html:text/html},
}

@misc{zlocha_improving_2019,
	title = {Improving {RetinaNet} for {CT} {Lesion} {Detection} with {Dense} {Masks} from {Weak} {RECIST} {Labels}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1906.02283},
	doi = {10.48550/ARXIV.1906.02283},
	abstract = {Accurate, automated lesion detection in Computed Tomography (CT) is an important yet challenging task due to the large variation of lesion types, sizes, locations and appearances. Recent work on CT lesion detection employs two-stage region proposal based methods trained with centroid or bounding-box annotations. We propose a highly accurate and efficient one-stage lesion detector, by re-designing a RetinaNet to meet the particular challenges in medical imaging. Specifically, we optimize the anchor configurations using a differential evolution search algorithm. For training, we leverage the response evaluation criteria in solid tumors (RECIST) annotation which are measured in clinical routine. We incorporate dense masks from weak RECIST labels, obtained automatically using GrabCut, into the training objective, which in combination with other advancements yields new state-of-the-art performance. We evaluate our method on the public DeepLesion benchmark, consisting of 32,735 lesions across the body. Our one-stage detector achieves a sensitivity of 90.77\% at 4 false positives per image, significantly outperforming the best reported methods by over 5\%.},
	urldate = {2024-08-25},
	publisher = {arXiv},
	author = {Zlocha, Martin and Dou, Qi and Glocker, Ben},
	year = {2019},
	note = {Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Image and Video Processing (eess.IV), Machine Learning (cs.LG)},
	annote = {Other
Accepted at MICCAI 2019},
}

@article{Rashidi_Fathabadi_Grantner_Shebrain_Abdel,
	title = {Autonomous sequential surgical skills assessment for the peg transfer task in a laparoscopic box-trainer system with three cameras},
	volume = {41},
	DOI = {10.1017/S0263574723000218},
	number = {6},
	journal = {Robotica},
	author = {Rashidi Fathabadi, Fatemeh and Grantner, Janos L. and Shebrain, Saad A and Abdel-Qader, Ikhlas},
	year = {2023},
	pages = {1837–1855},
}


@article{maciel_development_2008,
	title = {Development of the {VBLaST} $^{\textrm{™}}$ : a virtual basic laparoscopic skill trainer},
	volume = {4},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	issn = {1478-5951, 1478-596X},
	shorttitle = {Development of the {VBLaST} $^{\textrm{™}}$},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/rcs.185},
	doi = {10.1002/rcs.185},
	abstract = {Abstract
            
              Background
              The FLS training tool box has now been adopted by the Society of Gastrointestinal Endoscopic Surgeons (SAGES) as an official training tool for minimally invasive procedures.
            
            
              Methods
              To overcome the limitations of the physical FLS training tool box, we have developed a Virtual Basic Laparoscopic Skill Trainer (VBLaSTTM) system, which is a 3D simulator that will allow trainees to acquire basic laparoscopic skill.
            
            
              Results
              The outcome of this work is the development of an integrated visio‐haptic workstation environment including force feedback devices and a stereo display interface whereby trainees can practice on virtual versions of the FLS. Realistic graphical rendering and high fidelity haptic interactions are achieved.
            
            
              Conclusions
              Surgical skill training is a long and tedious process of acquiring fine motor skills. It is expected that residents would start on trainers such as VBLaSTTM and after reaching a certain level of competence would progress to the more complex trainers for training on specific surgical procedures. Copyright © 2008 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {2},
	urldate = {2024-08-26},
	journal = {The International Journal of Medical Robotics and Computer Assisted Surgery},
	author = {Maciel, Anderson and Liu, Youquan and Ahn, Woojin and Singh, T. Paul and Dunnican, Ward and De, Suvranu},
	month = jun,
	year = {2008},
	pages = {131--138},
	file = {Accepted Version:/Users/scsoc/Zotero/storage/45FF3LU3/Maciel et al. - 2008 - Development of the VBLaST ™  a virtual.pdf:application/pdf},
}

@article{matsumoto_laparoscopic_2022,
	title = {Laparoscopic surgical skill evaluation with motion capture and eyeglass gaze cameras: {A} pilot study},
	volume = {15},
	issn = {1758-5902, 1758-5910},
	shorttitle = {Laparoscopic surgical skill evaluation with motion capture and eyeglass gaze cameras},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/ases.13065},
	doi = {10.1111/ases.13065},
	abstract = {Abstract
            
              Introduction
              An eyeglass gaze camera and a skeletal coordinate camera without sensors attached to the operator's body were used to monitor gaze and movement during a simulated surgical procedure. These new devices have the potential to change skill assessment for laparoscopic surgery. The suitability of these devices for skill assessment was investigated.
            
            
              Material and Methods
              Six medical students, six intermediate surgeons, and four experts performed suturing tasks in a dry box. The tip positions of the instruments were identified from video recordings. Performance was evaluated based on instrument movement, gaze, and skeletal coordination.
            
            
              Results
              Task performance time and skeletal coordinates were not significantly different among skill levels. The total movement distance of the right instrument was significantly different depending on the skill level. The SD of the gaze coordinates was significantly different depending on skill level and was less for experts. The expert's gaze stayed in a small area with little blurring.
            
            
              Conclusions
              The SD of gaze point coordinates correlates with laparoscopic surgical skill level. These devices may facilitate objective intraoperative skill evaluation in future studies.},
	language = {en},
	number = {3},
	urldate = {2024-08-26},
	journal = {Asian Journal of Endoscopic Surgery},
	author = {Matsumoto, Shiro and Kawahira, Hiroshi and Oiwa, Kosuke and Maeda, Yoshitaka and Nozawa, Akio and Lefor, Alan Kawarai and Hosoya, Yoshinori and Sata, Naohiro},
	month = jul,
	year = {2022},
	pages = {619--628},
}


@misc{urology2023,
	title = {Medical {Education} {Leeds} - {Course}: {UBOOT} 22},
	url = {https://www.maxcourse.co.uk/medicaleducationleeds/guestCourseDetails.asp?cKey=1443},
	urldate = {2024-08-26},
	file = {Medical Education Leeds - Course\: UBOOT 22:/Users/scsoc/Zotero/storage/25RNHZMC/guestCourseDetails.html:text/html},
	year = {2023},
	author = {Medical Education Leeds},
}

@misc{herrera_luiscarlosgphkeypoint-annotation-tool_2024,
	title = {luiscarlosgph/keypoint-annotation-tool},
	copyright = {MIT},
	url = {https://github.com/luiscarlosgph/keypoint-annotation-tool},
	abstract = {Web app to annotate images for surgical tooltip localisation.},
	urldate = {2024-08-26},
	author = {Herrera, Luis C. Garcia Peraza},
	month = jun,
	year = {2024},
	note = {original-date: 2021-01-20T23:32:50Z},
}

@misc{choudhry_omarioscmsc-surgical-tool-tracking_2024,
	title = {omariosc/msc-surgical-tool-tracking},
	url = {https://github.com/omariosc/msc-surgical-tool-tracking},
	abstract = {MSc Code},
	urldate = {2024-08-26},
	author = {Choudhry, Omar},
	month = aug,
	year = {2024},
	note = {original-date: 2024-04-21T19:14:57Z},
}
